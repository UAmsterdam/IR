{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a21c96d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import sklearn_crfsuite\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from datasets import Dataset\n",
    "import pickle\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df0f13fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>index</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>topic_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EXECUTION VERSION</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>150705_0</td>\n",
       "      <td>1252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE BANK OF NOVA SCOTIA</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>150705_1</td>\n",
       "      <td>1252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>as Sole Bookrunner , Lead Arranger and Adminis...</td>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "      <td>150705_2</td>\n",
       "      <td>1252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- and-</td>\n",
       "      <td>B</td>\n",
       "      <td>3</td>\n",
       "      <td>150705_3</td>\n",
       "      <td>1252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BMO CAPITAL MARKETS</td>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "      <td>150705_4</td>\n",
       "      <td>1252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence label  index   sent_id  \\\n",
       "0                                  EXECUTION VERSION     B      0  150705_0   \n",
       "1                            THE BANK OF NOVA SCOTIA     B      1  150705_1   \n",
       "2  as Sole Bookrunner , Lead Arranger and Adminis...     B      2  150705_2   \n",
       "3                                             - and-     B      3  150705_3   \n",
       "4                                BMO CAPITAL MARKETS     B      4  150705_4   \n",
       "\n",
       "   topic_id  \n",
       "0      1252  \n",
       "1      1252  \n",
       "2      1252  \n",
       "3      1252  \n",
       "4      1252  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('due_dilligence_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae315b70",
   "metadata": {},
   "source": [
    "### Load the custom punkt tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff7ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "with open('custom_punkt_tokenizer.pkl', 'rb') as f:\n",
    "    loaded_tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468bc6ef",
   "metadata": {},
   "source": [
    "### Feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "655fedbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def extract_features_and_labels(sentence, label):\n",
    "    # Tokenize the sentence\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    # Generate n-grams (unigrams, bigrams, trigrams)\n",
    "    unigrams = list(ngrams(tokens, 1))\n",
    "    bigrams = list(ngrams(tokens, 2))\n",
    "    trigrams = list(ngrams(tokens, 3))\n",
    "    \n",
    "    # Generate features for each token\n",
    "    token_features = []\n",
    "    token_labels = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        features = {\n",
    "            'token': token,\n",
    "            'lower': token.lower(),\n",
    "            'is_first': i == 0,\n",
    "            'is_last': i == len(tokens) - 1,\n",
    "            'is_capitalized': token[0].isupper(),\n",
    "            'is_all_caps': token.isupper(),\n",
    "            'is_all_lower': token.islower(),\n",
    "            'prefix-1': token[0],\n",
    "            'prefix-2': token[:2],\n",
    "            'prefix-3': token[:3],\n",
    "            'suffix-1': token[-1],\n",
    "            'suffix-2': token[-2:],\n",
    "            'suffix-3': token[-3:],\n",
    "            'prev_token': '' if i == 0 else tokens[i - 1],\n",
    "            'next_token': '' if i == len(tokens) - 1 else tokens[i + 1],\n",
    "            'is_numeric': token.isdigit(),\n",
    "            'unigram': unigrams[i][0] if i < len(unigrams) else '',\n",
    "            'bigram': ' '.join(bigrams[i]) if i < len(bigrams) else '',\n",
    "            'trigram': ' '.join(trigrams[i]) if i < len(trigrams) else '',\n",
    "        }\n",
    "        token_features.append(features)\n",
    "        token_labels.append(label)\n",
    "        \n",
    "    return token_features, token_labels\n",
    "\n",
    "\n",
    "def process_text_and_extract_features(text, label):\n",
    "    # Segment the text into sentences using the custom tokenizer\n",
    "    sentences = loaded_tokenizer.tokenize(text)\n",
    "    \n",
    "    # Extract features for each sentence\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for sentence in sentences:\n",
    "        sentence_features, sentence_labels = extract_features_and_labels(sentence, label)\n",
    "        all_features.extend(sentence_features)\n",
    "        all_labels.extend(sentence_labels)\n",
    "    \n",
    "    return all_features, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f727d",
   "metadata": {},
   "source": [
    "## Training CRF model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c0cf5",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a65bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train function\n",
    "from itertools import chain\n",
    "\n",
    "def train_crf_model(train_data, test_data, model_save_dir, topic_id, fold):\n",
    "    ''' This function is training and saving CRF model for each fold.'''\n",
    "    \n",
    "    train_sentences = train_data['sentence']\n",
    "    train_labels = train_data['label']\n",
    "    \n",
    "    # Extract features and labels for each training\n",
    "    train_extracted = [process_text_and_extract_features(sentence, label) for sentence, label in zip(train_sentences, train_labels)]\n",
    "    \n",
    "    \n",
    "    X_train = [features for features, _ in train_extracted]\n",
    "    y_train = [labels for _, labels in train_extracted]\n",
    "\n",
    "    print(train_sentences[:2])\n",
    "    \n",
    "    print(\"First sentence features in training data:\", X_train[:2])\n",
    "    print(\"First sentence labels in training data:\", y_train[:2])\n",
    "\n",
    "#     esd\n",
    "    print(f\"Training model for topic {topic_id} for fold {fold}\")\n",
    "\n",
    "    crf = sklearn_crfsuite.CRF(algorithm='pa', c=0.1, pa_type=2, max_iterations=100,\n",
    "                               all_possible_transitions=True, verbose=True)\n",
    "    crf.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the trained CRF model for the topic\n",
    "    fold_model_dir = os.path.join(model_save_dir, topic_id)\n",
    "    \n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(fold_model_dir, exist_ok=True)\n",
    "\n",
    "    # Define the full path for the model file\n",
    "    fold_model_path = os.path.join(fold_model_dir, f\"{topic_id}_crf_model_{fold}.pkl\")\n",
    "\n",
    "    # Save the model\n",
    "    with open(fold_model_path, \"wb\") as model_file:\n",
    "        pickle.dump(crf, model_file)\n",
    "\n",
    "    print(f\"CRF model saved for topic {topic_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f140261",
   "metadata": {},
   "source": [
    "### Functions to load the data using predefined folds for 5 folds cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a3d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = 'core/qrels/'\n",
    "\n",
    "def map_doc(row):\n",
    "    return row['sent_id'].split('_')[0]\n",
    "\n",
    "def read_split(file_):\n",
    "    return [el for el in open(file_).read().split('\\n') if el != '']\n",
    "\n",
    "def map_doc(row):\n",
    "    return row['sent_id'].split('_')[0]\n",
    "\n",
    "def stratify(df):\n",
    "    g = df.groupby('label')\n",
    "    return df, g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\n",
    "\n",
    "# List of folder names you want to process\n",
    "# folders_to_process = ['1272', '1474','1238', '1275', '1239', '1520', '1509', '1240', '1308', '1319', '1439', \n",
    "#                  '1267', '1242', '1462', '1265', '1444', '1312', '1244', '1243', '1468', '1309', '1524', \n",
    "#                  '1247', '1440', '1251', '1249', '1248', '1262', '1250', '1252', '1245', '1512', '1498', \n",
    "#                  '1601', '1443', '1086', '1551', '1253', '1320', '1304', '1469', '1611', '1300', '1489', \n",
    "#                  '1500', '1261', '1318', '1460', '1475', '1321']\n",
    "\n",
    "folders_to_process = [ '1250']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905bd48d",
   "metadata": {},
   "source": [
    "### Data prepration and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffa368e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for fold: 0\n",
      "label\n",
      "1    25\n",
      "B    25\n",
      "Name: count, dtype: int64 label\n",
      "B    179377\n",
      "1        25\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    106\n",
      "B    106\n",
      "Name: count, dtype: int64 label\n",
      "B    752019\n",
      "1       106\n",
      "Name: count, dtype: int64\n",
      "32413173          EXECUTION VERSION\n",
      "32413174    THE BANK OF NOVA SCOTIA\n",
      "Name: sentence, dtype: object\n",
      "First sentence features in training data: [[{'token': 'EXECUTION', 'lower': 'execution', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'E', 'prefix-2': 'EX', 'prefix-3': 'EXE', 'suffix-1': 'N', 'suffix-2': 'ON', 'suffix-3': 'ION', 'prev_token': '', 'next_token': 'VERSION', 'is_numeric': False, 'unigram': 'EXECUTION', 'bigram': 'EXECUTION VERSION', 'trigram': ''}, {'token': 'VERSION', 'lower': 'version', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'V', 'prefix-2': 'VE', 'prefix-3': 'VER', 'suffix-1': 'N', 'suffix-2': 'ON', 'suffix-3': 'ION', 'prev_token': 'EXECUTION', 'next_token': '', 'is_numeric': False, 'unigram': 'VERSION', 'bigram': '', 'trigram': ''}], [{'token': 'THE', 'lower': 'the', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'T', 'prefix-2': 'TH', 'prefix-3': 'THE', 'suffix-1': 'E', 'suffix-2': 'HE', 'suffix-3': 'THE', 'prev_token': '', 'next_token': 'BANK', 'is_numeric': False, 'unigram': 'THE', 'bigram': 'THE BANK', 'trigram': 'THE BANK OF'}, {'token': 'BANK', 'lower': 'bank', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'B', 'prefix-2': 'BA', 'prefix-3': 'BAN', 'suffix-1': 'K', 'suffix-2': 'NK', 'suffix-3': 'ANK', 'prev_token': 'THE', 'next_token': 'OF', 'is_numeric': False, 'unigram': 'BANK', 'bigram': 'BANK OF', 'trigram': 'BANK OF NOVA'}, {'token': 'OF', 'lower': 'of', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'O', 'prefix-2': 'OF', 'prefix-3': 'OF', 'suffix-1': 'F', 'suffix-2': 'OF', 'suffix-3': 'OF', 'prev_token': 'BANK', 'next_token': 'NOVA', 'is_numeric': False, 'unigram': 'OF', 'bigram': 'OF NOVA', 'trigram': 'OF NOVA SCOTIA'}, {'token': 'NOVA', 'lower': 'nova', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'N', 'prefix-2': 'NO', 'prefix-3': 'NOV', 'suffix-1': 'A', 'suffix-2': 'VA', 'suffix-3': 'OVA', 'prev_token': 'OF', 'next_token': 'SCOTIA', 'is_numeric': False, 'unigram': 'NOVA', 'bigram': 'NOVA SCOTIA', 'trigram': ''}, {'token': 'SCOTIA', 'lower': 'scotia', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'S', 'prefix-2': 'SC', 'prefix-3': 'SCO', 'suffix-1': 'A', 'suffix-2': 'IA', 'suffix-3': 'TIA', 'prev_token': 'NOVA', 'next_token': '', 'is_numeric': False, 'unigram': 'SCOTIA', 'bigram': '', 'trigram': ''}]]\n",
      "First sentence labels in training data: [['B', 'B'], ['B', 'B', 'B', 'B', 'B']]\n",
      "Training model for topic 1250 for fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|█| 752125/752125 [03:59<00:00, 3140.20it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 2807308\n",
      "Seconds required: 62.713\n",
      "\n",
      "Passive Aggressive\n",
      "type: 2\n",
      "c: 0.100000\n",
      "error_sensitive: 1\n",
      "averaging: 1\n",
      "max_iterations: 100\n",
      "epsilon: 0.000000\n",
      "\n",
      "Iter 1   time=6.46  loss=2948.28  feature_norm=1.20\n",
      "Iter 2   time=6.16  loss=2316.47  feature_norm=1.67\n",
      "Iter 3   time=6.22  loss=2015.79  feature_norm=1.99\n",
      "Iter 4   time=6.73  loss=1449.58  feature_norm=2.20\n",
      "Iter 5   time=6.47  loss=1181.80  feature_norm=2.37\n",
      "Iter 6   time=6.36  loss=1072.01  feature_norm=2.49\n",
      "Iter 7   time=6.25  loss=913.89   feature_norm=2.59\n",
      "Iter 8   time=6.10  loss=732.78   feature_norm=2.68\n",
      "Iter 9   time=6.33  loss=747.02   feature_norm=2.75\n",
      "Iter 10  time=6.11  loss=594.49   feature_norm=2.79\n",
      "Iter 11  time=6.14  loss=582.27   feature_norm=2.84\n",
      "Iter 12  time=6.11  loss=464.35   feature_norm=2.88\n",
      "Iter 13  time=6.34  loss=609.99   feature_norm=2.94\n",
      "Iter 14  time=6.42  loss=365.12   feature_norm=2.96\n",
      "Iter 15  time=6.06  loss=356.07   feature_norm=3.00\n",
      "Iter 16  time=6.09  loss=446.34   feature_norm=3.04\n",
      "Iter 17  time=6.14  loss=281.41   feature_norm=3.06\n",
      "Iter 18  time=6.18  loss=455.98   feature_norm=3.11\n",
      "Iter 19  time=6.34  loss=275.01   feature_norm=3.13\n",
      "Iter 20  time=6.41  loss=453.10   feature_norm=3.17\n",
      "Iter 21  time=6.25  loss=230.99   feature_norm=3.20\n",
      "Iter 22  time=6.23  loss=379.96   feature_norm=3.23\n",
      "Iter 23  time=6.39  loss=216.03   feature_norm=3.25\n",
      "Iter 24  time=6.24  loss=298.67   feature_norm=3.28\n",
      "Iter 25  time=6.24  loss=242.82   feature_norm=3.30\n",
      "Iter 26  time=6.17  loss=395.01   feature_norm=3.33\n",
      "Iter 27  time=6.29  loss=286.70   feature_norm=3.35\n",
      "Iter 28  time=6.19  loss=301.05   feature_norm=3.37\n",
      "Iter 29  time=6.28  loss=291.76   feature_norm=3.40\n",
      "Iter 30  time=6.10  loss=268.44   feature_norm=3.42\n",
      "Iter 31  time=6.12  loss=157.32   feature_norm=3.43\n",
      "Iter 32  time=6.45  loss=118.21   feature_norm=3.45\n",
      "Iter 33  time=6.34  loss=301.32   feature_norm=3.47\n",
      "Iter 34  time=6.16  loss=181.93   feature_norm=3.48\n",
      "Iter 35  time=6.15  loss=232.37   feature_norm=3.50\n",
      "Iter 36  time=6.16  loss=240.59   feature_norm=3.51\n",
      "Iter 37  time=6.39  loss=82.27    feature_norm=3.52\n",
      "Iter 38  time=6.20  loss=128.50   feature_norm=3.53\n",
      "Iter 39  time=6.13  loss=296.78   feature_norm=3.56\n",
      "Iter 40  time=6.41  loss=238.46   feature_norm=3.57\n",
      "Iter 41  time=6.28  loss=225.22   feature_norm=3.59\n",
      "Iter 42  time=6.26  loss=42.86    feature_norm=3.59\n",
      "Iter 43  time=6.20  loss=266.18   feature_norm=3.61\n",
      "Iter 44  time=6.15  loss=196.25   feature_norm=3.62\n",
      "Iter 45  time=6.29  loss=213.85   feature_norm=3.65\n",
      "Iter 46  time=6.34  loss=266.77   feature_norm=3.67\n",
      "Iter 47  time=6.15  loss=255.74   feature_norm=3.69\n",
      "Iter 48  time=6.22  loss=248.90   feature_norm=3.71\n",
      "Iter 49  time=6.18  loss=316.02   feature_norm=3.72\n",
      "Iter 50  time=6.58  loss=239.97   feature_norm=3.74\n",
      "Iter 51  time=6.50  loss=214.02   feature_norm=3.75\n",
      "Iter 52  time=6.18  loss=210.25   feature_norm=3.76\n",
      "Iter 53  time=6.13  loss=143.65   feature_norm=3.77\n",
      "Iter 54  time=6.15  loss=256.13   feature_norm=3.79\n",
      "Iter 55  time=6.16  loss=177.66   feature_norm=3.80\n",
      "Iter 56  time=6.25  loss=109.55   feature_norm=3.80\n",
      "Iter 57  time=6.21  loss=108.82   feature_norm=3.81\n",
      "Iter 58  time=6.31  loss=168.19   feature_norm=3.82\n",
      "Iter 59  time=6.08  loss=150.33   feature_norm=3.83\n",
      "Iter 60  time=6.38  loss=71.29    feature_norm=3.83\n",
      "Iter 61  time=6.22  loss=59.79    feature_norm=3.83\n",
      "Iter 62  time=6.23  loss=51.07    feature_norm=3.83\n",
      "Iter 63  time=6.27  loss=48.08    feature_norm=3.84\n",
      "Iter 64  time=6.18  loss=137.45   feature_norm=3.85\n",
      "Iter 65  time=6.30  loss=141.33   feature_norm=3.86\n",
      "Iter 66  time=6.08  loss=50.45    feature_norm=3.86\n",
      "Iter 67  time=6.37  loss=78.66    feature_norm=3.87\n",
      "Iter 68  time=6.14  loss=145.81   feature_norm=3.89\n",
      "Iter 69  time=6.50  loss=82.12    feature_norm=3.90\n",
      "Iter 70  time=6.09  loss=143.60   feature_norm=3.91\n",
      "Iter 71  time=6.08  loss=78.82    feature_norm=3.92\n",
      "Iter 72  time=6.32  loss=185.53   feature_norm=3.93\n",
      "Iter 73  time=6.65  loss=121.37   feature_norm=3.93\n",
      "Iter 74  time=6.61  loss=45.26    feature_norm=3.93\n",
      "Iter 75  time=6.41  loss=23.33    feature_norm=3.94\n",
      "Iter 76  time=6.50  loss=122.09   feature_norm=3.94\n",
      "Iter 77  time=6.22  loss=44.35    feature_norm=3.94\n",
      "Iter 78  time=6.59  loss=126.08   feature_norm=3.95\n",
      "Iter 79  time=6.23  loss=135.32   feature_norm=3.96\n",
      "Iter 80  time=6.30  loss=6.10     feature_norm=3.96\n",
      "Iter 81  time=6.28  loss=51.13    feature_norm=3.96\n",
      "Iter 82  time=6.26  loss=8.76     feature_norm=3.96\n",
      "Iter 83  time=6.21  loss=8.82     feature_norm=3.96\n",
      "Iter 84  time=6.26  loss=153.47   feature_norm=3.97\n",
      "Iter 85  time=6.40  loss=95.27    feature_norm=3.97\n",
      "Iter 86  time=6.14  loss=98.39    feature_norm=3.99\n",
      "Iter 87  time=6.58  loss=35.18    feature_norm=3.99\n",
      "Iter 88  time=6.25  loss=54.20    feature_norm=4.00\n",
      "Iter 89  time=6.17  loss=120.16   feature_norm=4.00\n",
      "Iter 90  time=6.12  loss=195.18   feature_norm=4.02\n",
      "Iter 91  time=6.21  loss=184.35   feature_norm=4.02\n",
      "Iter 92  time=6.25  loss=55.57    feature_norm=4.03\n",
      "Iter 93  time=6.39  loss=37.69    feature_norm=4.03\n",
      "Iter 94  time=6.22  loss=8.76     feature_norm=4.03\n",
      "Iter 95  time=6.25  loss=30.02    feature_norm=4.03\n",
      "Iter 96  time=6.41  loss=33.03    feature_norm=4.03\n",
      "Iter 97  time=6.38  loss=79.45    feature_norm=4.03\n",
      "Iter 98  time=6.20  loss=37.07    feature_norm=4.03\n",
      "Iter 99  time=6.25  loss=44.03    feature_norm=4.04\n",
      "Iter 100 time=6.19  loss=48.91    feature_norm=4.04\n",
      "Total seconds required for training: 627.067\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 72595 (2807308)\n",
      "Number of active attributes: 63792 (2798504)\n",
      "Number of active labels: 2 (2)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.189\n",
      "\n",
      "CRF model saved for topic 1250\n",
      "['B', 'B', 'B']\n",
      "['B', 'B', 'B']\n",
      "Precision: 0.9869024224054338, Recall: 0.8818022230065522, F1: 0.9313962871619642\n",
      "Topic 1250 - Avg Precision: 0.9869, Avg Recall: 0.8818, Avg F1-Score: 0.9314\n",
      "for fold: 1\n",
      "label\n",
      "1    26\n",
      "B    26\n",
      "Name: count, dtype: int64 label\n",
      "B    185433\n",
      "1        26\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    105\n",
      "B    105\n",
      "Name: count, dtype: int64 label\n",
      "B    745963\n",
      "1       105\n",
      "Name: count, dtype: int64\n",
      "32413173          EXECUTION VERSION\n",
      "32413174    THE BANK OF NOVA SCOTIA\n",
      "Name: sentence, dtype: object\n",
      "First sentence features in training data: [[{'token': 'EXECUTION', 'lower': 'execution', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'E', 'prefix-2': 'EX', 'prefix-3': 'EXE', 'suffix-1': 'N', 'suffix-2': 'ON', 'suffix-3': 'ION', 'prev_token': '', 'next_token': 'VERSION', 'is_numeric': False, 'unigram': 'EXECUTION', 'bigram': 'EXECUTION VERSION', 'trigram': ''}, {'token': 'VERSION', 'lower': 'version', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'V', 'prefix-2': 'VE', 'prefix-3': 'VER', 'suffix-1': 'N', 'suffix-2': 'ON', 'suffix-3': 'ION', 'prev_token': 'EXECUTION', 'next_token': '', 'is_numeric': False, 'unigram': 'VERSION', 'bigram': '', 'trigram': ''}], [{'token': 'THE', 'lower': 'the', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'T', 'prefix-2': 'TH', 'prefix-3': 'THE', 'suffix-1': 'E', 'suffix-2': 'HE', 'suffix-3': 'THE', 'prev_token': '', 'next_token': 'BANK', 'is_numeric': False, 'unigram': 'THE', 'bigram': 'THE BANK', 'trigram': 'THE BANK OF'}, {'token': 'BANK', 'lower': 'bank', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'B', 'prefix-2': 'BA', 'prefix-3': 'BAN', 'suffix-1': 'K', 'suffix-2': 'NK', 'suffix-3': 'ANK', 'prev_token': 'THE', 'next_token': 'OF', 'is_numeric': False, 'unigram': 'BANK', 'bigram': 'BANK OF', 'trigram': 'BANK OF NOVA'}, {'token': 'OF', 'lower': 'of', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'O', 'prefix-2': 'OF', 'prefix-3': 'OF', 'suffix-1': 'F', 'suffix-2': 'OF', 'suffix-3': 'OF', 'prev_token': 'BANK', 'next_token': 'NOVA', 'is_numeric': False, 'unigram': 'OF', 'bigram': 'OF NOVA', 'trigram': 'OF NOVA SCOTIA'}, {'token': 'NOVA', 'lower': 'nova', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'N', 'prefix-2': 'NO', 'prefix-3': 'NOV', 'suffix-1': 'A', 'suffix-2': 'VA', 'suffix-3': 'OVA', 'prev_token': 'OF', 'next_token': 'SCOTIA', 'is_numeric': False, 'unigram': 'NOVA', 'bigram': 'NOVA SCOTIA', 'trigram': ''}, {'token': 'SCOTIA', 'lower': 'scotia', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'S', 'prefix-2': 'SC', 'prefix-3': 'SCO', 'suffix-1': 'A', 'suffix-2': 'IA', 'suffix-3': 'TIA', 'prev_token': 'NOVA', 'next_token': '', 'is_numeric': False, 'unigram': 'SCOTIA', 'bigram': '', 'trigram': ''}]]\n",
      "First sentence labels in training data: [['B', 'B'], ['B', 'B', 'B', 'B', 'B']]\n",
      "Training model for topic 1250 for fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|█| 746068/746068 [04:18<00:00, 2881.39it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 2859236\n",
      "Seconds required: 60.677\n",
      "\n",
      "Passive Aggressive\n",
      "type: 2\n",
      "c: 0.100000\n",
      "error_sensitive: 1\n",
      "averaging: 1\n",
      "max_iterations: 100\n",
      "epsilon: 0.000000\n",
      "\n",
      "Iter 1   time=7.26  loss=3164.31  feature_norm=1.20\n",
      "Iter 2   time=6.66  loss=2173.70  feature_norm=1.66\n",
      "Iter 3   time=6.57  loss=1767.42  feature_norm=1.95\n",
      "Iter 4   time=6.50  loss=1489.02  feature_norm=2.17\n",
      "Iter 5   time=6.62  loss=1299.38  feature_norm=2.35\n",
      "Iter 6   time=6.52  loss=947.41   feature_norm=2.48\n",
      "Iter 7   time=6.54  loss=1071.63  feature_norm=2.59\n",
      "Iter 8   time=6.46  loss=719.61   feature_norm=2.68\n",
      "Iter 9   time=6.84  loss=934.32   feature_norm=2.78\n",
      "Iter 10  time=6.78  loss=789.49   feature_norm=2.88\n",
      "Iter 11  time=6.68  loss=620.79   feature_norm=2.95\n",
      "Iter 12  time=6.56  loss=590.77   feature_norm=2.99\n",
      "Iter 13  time=6.54  loss=454.22   feature_norm=3.04\n",
      "Iter 14  time=6.57  loss=557.06   feature_norm=3.11\n",
      "Iter 15  time=6.57  loss=523.78   feature_norm=3.16\n",
      "Iter 16  time=6.63  loss=499.64   feature_norm=3.21\n",
      "Iter 17  time=6.57  loss=519.40   feature_norm=3.26\n",
      "Iter 18  time=7.05  loss=458.67   feature_norm=3.31\n",
      "Iter 19  time=6.65  loss=309.17   feature_norm=3.34\n",
      "Iter 20  time=6.64  loss=463.02   feature_norm=3.37\n",
      "Iter 21  time=6.43  loss=374.52   feature_norm=3.41\n",
      "Iter 22  time=6.59  loss=265.53   feature_norm=3.44\n",
      "Iter 23  time=6.46  loss=242.85   feature_norm=3.45\n",
      "Iter 24  time=6.58  loss=349.43   feature_norm=3.48\n",
      "Iter 25  time=6.61  loss=242.29   feature_norm=3.50\n",
      "Iter 26  time=6.58  loss=272.34   feature_norm=3.52\n",
      "Iter 27  time=6.84  loss=203.30   feature_norm=3.54\n",
      "Iter 28  time=6.50  loss=316.56   feature_norm=3.58\n",
      "Iter 29  time=6.49  loss=292.40   feature_norm=3.60\n",
      "Iter 30  time=6.53  loss=243.21   feature_norm=3.61\n",
      "Iter 31  time=6.65  loss=196.69   feature_norm=3.63\n",
      "Iter 32  time=6.76  loss=267.65   feature_norm=3.65\n",
      "Iter 33  time=6.53  loss=214.56   feature_norm=3.66\n",
      "Iter 34  time=6.57  loss=268.87   feature_norm=3.68\n",
      "Iter 35  time=6.76  loss=211.30   feature_norm=3.69\n",
      "Iter 36  time=6.63  loss=148.40   feature_norm=3.70\n",
      "Iter 37  time=6.61  loss=268.95   feature_norm=3.72\n",
      "Iter 38  time=6.49  loss=217.42   feature_norm=3.73\n",
      "Iter 39  time=6.49  loss=265.08   feature_norm=3.74\n",
      "Iter 40  time=6.50  loss=210.31   feature_norm=3.75\n",
      "Iter 41  time=6.45  loss=265.37   feature_norm=3.77\n",
      "Iter 42  time=6.50  loss=77.62    feature_norm=3.77\n",
      "Iter 43  time=6.54  loss=102.26   feature_norm=3.78\n",
      "Iter 44  time=7.01  loss=211.12   feature_norm=3.80\n",
      "Iter 45  time=6.56  loss=193.54   feature_norm=3.81\n",
      "Iter 46  time=6.55  loss=103.02   feature_norm=3.81\n",
      "Iter 47  time=6.67  loss=152.75   feature_norm=3.83\n",
      "Iter 48  time=6.58  loss=101.33   feature_norm=3.84\n",
      "Iter 49  time=6.48  loss=157.33   feature_norm=3.85\n",
      "Iter 50  time=6.47  loss=138.26   feature_norm=3.87\n",
      "Iter 51  time=6.56  loss=112.21   feature_norm=3.87\n",
      "Iter 52  time=6.63  loss=81.17    feature_norm=3.88\n",
      "Iter 53  time=6.73  loss=101.34   feature_norm=3.88\n",
      "Iter 54  time=6.75  loss=14.41    feature_norm=3.89\n",
      "Iter 55  time=6.57  loss=185.94   feature_norm=3.89\n",
      "Iter 56  time=6.56  loss=245.48   feature_norm=3.91\n",
      "Iter 57  time=6.59  loss=90.20    feature_norm=3.92\n",
      "Iter 58  time=6.71  loss=47.73    feature_norm=3.92\n",
      "Iter 59  time=6.44  loss=171.63   feature_norm=3.94\n",
      "Iter 60  time=6.55  loss=226.52   feature_norm=3.95\n",
      "Iter 61  time=6.73  loss=139.09   feature_norm=3.96\n",
      "Iter 62  time=6.78  loss=123.99   feature_norm=3.96\n",
      "Iter 63  time=6.81  loss=130.74   feature_norm=3.98\n",
      "Iter 64  time=6.53  loss=138.67   feature_norm=3.99\n",
      "Iter 65  time=6.47  loss=119.86   feature_norm=3.99\n",
      "Iter 66  time=6.62  loss=87.55    feature_norm=4.00\n",
      "Iter 67  time=6.56  loss=163.78   feature_norm=4.01\n",
      "Iter 68  time=6.48  loss=197.96   feature_norm=4.03\n",
      "Iter 69  time=6.51  loss=46.15    feature_norm=4.03\n",
      "Iter 70  time=6.62  loss=106.29   feature_norm=4.03\n",
      "Iter 71  time=6.65  loss=135.76   feature_norm=4.05\n",
      "Iter 72  time=6.63  loss=248.86   feature_norm=4.07\n",
      "Iter 73  time=6.60  loss=219.49   feature_norm=4.09\n",
      "Iter 74  time=6.40  loss=136.13   feature_norm=4.09\n",
      "Iter 75  time=6.36  loss=131.48   feature_norm=4.12\n",
      "Iter 76  time=6.53  loss=137.33   feature_norm=4.13\n",
      "Iter 77  time=6.39  loss=59.43    feature_norm=4.13\n",
      "Iter 78  time=6.44  loss=95.88    feature_norm=4.15\n",
      "Iter 79  time=6.73  loss=149.91   feature_norm=4.15\n",
      "Iter 80  time=6.43  loss=104.98   feature_norm=4.16\n",
      "Iter 81  time=6.44  loss=127.80   feature_norm=4.16\n",
      "Iter 82  time=6.39  loss=106.97   feature_norm=4.17\n",
      "Iter 83  time=6.50  loss=97.65    feature_norm=4.17\n",
      "Iter 84  time=6.37  loss=136.43   feature_norm=4.18\n",
      "Iter 85  time=6.51  loss=52.92    feature_norm=4.19\n",
      "Iter 86  time=6.58  loss=33.26    feature_norm=4.19\n",
      "Iter 87  time=6.62  loss=29.85    feature_norm=4.19\n",
      "Iter 88  time=6.66  loss=214.16   feature_norm=4.21\n",
      "Iter 89  time=6.58  loss=112.77   feature_norm=4.21\n",
      "Iter 90  time=6.56  loss=108.28   feature_norm=4.21\n",
      "Iter 91  time=6.52  loss=84.96    feature_norm=4.22\n",
      "Iter 92  time=6.58  loss=102.67   feature_norm=4.23\n",
      "Iter 93  time=6.45  loss=173.20   feature_norm=4.24\n",
      "Iter 94  time=6.40  loss=126.62   feature_norm=4.25\n",
      "Iter 95  time=6.35  loss=63.21    feature_norm=4.25\n",
      "Iter 96  time=6.77  loss=102.80   feature_norm=4.26\n",
      "Iter 97  time=6.59  loss=94.16    feature_norm=4.26\n",
      "Iter 98  time=6.62  loss=99.57    feature_norm=4.26\n",
      "Iter 99  time=6.45  loss=183.75   feature_norm=4.27\n",
      "Iter 100 time=6.56  loss=117.28   feature_norm=4.28\n",
      "Total seconds required for training: 658.504\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 73969 (2859236)\n",
      "Number of active attributes: 65317 (2850584)\n",
      "Number of active labels: 2 (2)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.183\n",
      "\n",
      "CRF model saved for topic 1250\n",
      "['B', 'B', 'B']\n",
      "['B', 'B', 'B']\n",
      "Precision: 0.9745718728725625, Recall: 0.8372715109954206, F1: 0.9007189268999631\n",
      "Topic 1250 - Avg Precision: 0.9746, Avg Recall: 0.8373, Avg F1-Score: 0.9007\n",
      "for fold: 2\n",
      "label\n",
      "1    33\n",
      "B    33\n",
      "Name: count, dtype: int64 label\n",
      "B    211644\n",
      "1        33\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    98\n",
      "B    98\n",
      "Name: count, dtype: int64 label\n",
      "B    719752\n",
      "1        98\n",
      "Name: count, dtype: int64\n",
      "32413173          EXECUTION VERSION\n",
      "32413174    THE BANK OF NOVA SCOTIA\n",
      "Name: sentence, dtype: object\n",
      "First sentence features in training data: [[{'token': 'EXECUTION', 'lower': 'execution', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'E', 'prefix-2': 'EX', 'prefix-3': 'EXE', 'suffix-1': 'N', 'suffix-2': 'ON', 'suffix-3': 'ION', 'prev_token': '', 'next_token': 'VERSION', 'is_numeric': False, 'unigram': 'EXECUTION', 'bigram': 'EXECUTION VERSION', 'trigram': ''}, {'token': 'VERSION', 'lower': 'version', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'V', 'prefix-2': 'VE', 'prefix-3': 'VER', 'suffix-1': 'N', 'suffix-2': 'ON', 'suffix-3': 'ION', 'prev_token': 'EXECUTION', 'next_token': '', 'is_numeric': False, 'unigram': 'VERSION', 'bigram': '', 'trigram': ''}], [{'token': 'THE', 'lower': 'the', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'T', 'prefix-2': 'TH', 'prefix-3': 'THE', 'suffix-1': 'E', 'suffix-2': 'HE', 'suffix-3': 'THE', 'prev_token': '', 'next_token': 'BANK', 'is_numeric': False, 'unigram': 'THE', 'bigram': 'THE BANK', 'trigram': 'THE BANK OF'}, {'token': 'BANK', 'lower': 'bank', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'B', 'prefix-2': 'BA', 'prefix-3': 'BAN', 'suffix-1': 'K', 'suffix-2': 'NK', 'suffix-3': 'ANK', 'prev_token': 'THE', 'next_token': 'OF', 'is_numeric': False, 'unigram': 'BANK', 'bigram': 'BANK OF', 'trigram': 'BANK OF NOVA'}, {'token': 'OF', 'lower': 'of', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'O', 'prefix-2': 'OF', 'prefix-3': 'OF', 'suffix-1': 'F', 'suffix-2': 'OF', 'suffix-3': 'OF', 'prev_token': 'BANK', 'next_token': 'NOVA', 'is_numeric': False, 'unigram': 'OF', 'bigram': 'OF NOVA', 'trigram': 'OF NOVA SCOTIA'}, {'token': 'NOVA', 'lower': 'nova', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'N', 'prefix-2': 'NO', 'prefix-3': 'NOV', 'suffix-1': 'A', 'suffix-2': 'VA', 'suffix-3': 'OVA', 'prev_token': 'OF', 'next_token': 'SCOTIA', 'is_numeric': False, 'unigram': 'NOVA', 'bigram': 'NOVA SCOTIA', 'trigram': ''}, {'token': 'SCOTIA', 'lower': 'scotia', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'S', 'prefix-2': 'SC', 'prefix-3': 'SCO', 'suffix-1': 'A', 'suffix-2': 'IA', 'suffix-3': 'TIA', 'prev_token': 'NOVA', 'next_token': '', 'is_numeric': False, 'unigram': 'SCOTIA', 'bigram': '', 'trigram': ''}]]\n",
      "First sentence labels in training data: [['B', 'B'], ['B', 'B', 'B', 'B', 'B']]\n",
      "Training model for topic 1250 for fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|█| 719850/719850 [04:10<00:00, 2872.96it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 2768289\n",
      "Seconds required: 66.560\n",
      "\n",
      "Passive Aggressive\n",
      "type: 2\n",
      "c: 0.100000\n",
      "error_sensitive: 1\n",
      "averaging: 1\n",
      "max_iterations: 100\n",
      "epsilon: 0.000000\n",
      "\n",
      "Iter 1   time=6.88  loss=2877.50  feature_norm=1.15\n",
      "Iter 2   time=6.48  loss=1977.74  feature_norm=1.57\n",
      "Iter 3   time=6.39  loss=1688.66  feature_norm=1.87\n",
      "Iter 4   time=6.39  loss=1432.63  feature_norm=2.12\n",
      "Iter 5   time=6.32  loss=1247.31  feature_norm=2.30\n",
      "Iter 6   time=6.43  loss=849.14   feature_norm=2.42\n",
      "Iter 7   time=6.59  loss=779.25   feature_norm=2.50\n",
      "Iter 8   time=6.81  loss=616.31   feature_norm=2.57\n",
      "Iter 9   time=6.35  loss=768.29   feature_norm=2.67\n",
      "Iter 10  time=6.46  loss=653.81   feature_norm=2.75\n",
      "Iter 11  time=6.48  loss=468.06   feature_norm=2.81\n",
      "Iter 12  time=6.22  loss=397.39   feature_norm=2.84\n",
      "Iter 13  time=6.34  loss=309.91   feature_norm=2.88\n",
      "Iter 14  time=6.25  loss=408.33   feature_norm=2.91\n",
      "Iter 15  time=6.33  loss=520.71   feature_norm=2.98\n",
      "Iter 16  time=6.59  loss=409.86   feature_norm=3.01\n",
      "Iter 17  time=6.41  loss=317.52   feature_norm=3.03\n",
      "Iter 18  time=6.65  loss=219.94   feature_norm=3.05\n",
      "Iter 19  time=6.39  loss=428.67   feature_norm=3.09\n",
      "Iter 20  time=6.35  loss=370.63   feature_norm=3.12\n",
      "Iter 21  time=6.32  loss=154.63   feature_norm=3.14\n",
      "Iter 22  time=6.30  loss=284.18   feature_norm=3.18\n",
      "Iter 23  time=6.44  loss=340.13   feature_norm=3.21\n",
      "Iter 24  time=6.48  loss=279.69   feature_norm=3.23\n",
      "Iter 25  time=6.40  loss=318.78   feature_norm=3.26\n",
      "Iter 26  time=6.52  loss=223.80   feature_norm=3.29\n",
      "Iter 27  time=6.45  loss=233.00   feature_norm=3.31\n",
      "Iter 28  time=6.40  loss=218.37   feature_norm=3.32\n",
      "Iter 29  time=6.33  loss=137.92   feature_norm=3.33\n",
      "Iter 30  time=6.46  loss=231.10   feature_norm=3.35\n",
      "Iter 31  time=6.58  loss=169.39   feature_norm=3.37\n",
      "Iter 32  time=6.53  loss=160.03   feature_norm=3.37\n",
      "Iter 33  time=6.74  loss=261.07   feature_norm=3.39\n",
      "Iter 34  time=6.75  loss=217.56   feature_norm=3.41\n",
      "Iter 35  time=6.70  loss=277.29   feature_norm=3.44\n",
      "Iter 36  time=6.46  loss=164.20   feature_norm=3.45\n",
      "Iter 37  time=6.40  loss=201.51   feature_norm=3.47\n",
      "Iter 38  time=6.43  loss=189.42   feature_norm=3.49\n",
      "Iter 39  time=6.40  loss=183.98   feature_norm=3.51\n",
      "Iter 40  time=6.51  loss=312.87   feature_norm=3.54\n",
      "Iter 41  time=6.45  loss=147.97   feature_norm=3.55\n",
      "Iter 42  time=6.58  loss=185.50   feature_norm=3.56\n",
      "Iter 43  time=6.44  loss=115.34   feature_norm=3.57\n",
      "Iter 44  time=6.61  loss=179.93   feature_norm=3.58\n",
      "Iter 45  time=6.56  loss=232.09   feature_norm=3.60\n",
      "Iter 46  time=6.48  loss=215.08   feature_norm=3.61\n",
      "Iter 47  time=6.32  loss=134.66   feature_norm=3.63\n",
      "Iter 48  time=6.34  loss=83.24    feature_norm=3.64\n",
      "Iter 49  time=6.37  loss=199.37   feature_norm=3.64\n",
      "Iter 50  time=6.31  loss=215.33   feature_norm=3.66\n",
      "Iter 51  time=6.40  loss=109.28   feature_norm=3.66\n",
      "Iter 52  time=6.46  loss=94.39    feature_norm=3.67\n",
      "Iter 53  time=6.91  loss=85.84    feature_norm=3.68\n",
      "Iter 54  time=6.35  loss=259.75   feature_norm=3.70\n",
      "Iter 55  time=6.33  loss=192.90   feature_norm=3.71\n",
      "Iter 56  time=6.40  loss=223.12   feature_norm=3.73\n",
      "Iter 57  time=6.39  loss=86.31    feature_norm=3.73\n",
      "Iter 58  time=6.42  loss=80.36    feature_norm=3.74\n",
      "Iter 59  time=6.43  loss=37.67    feature_norm=3.74\n",
      "Iter 60  time=6.37  loss=97.96    feature_norm=3.74\n",
      "Iter 61  time=6.49  loss=174.20   feature_norm=3.76\n",
      "Iter 62  time=6.73  loss=120.07   feature_norm=3.77\n",
      "Iter 63  time=6.44  loss=93.85    feature_norm=3.77\n",
      "Iter 64  time=6.39  loss=187.77   feature_norm=3.79\n",
      "Iter 65  time=6.28  loss=99.67    feature_norm=3.80\n",
      "Iter 66  time=6.66  loss=137.74   feature_norm=3.81\n",
      "Iter 67  time=6.58  loss=58.38    feature_norm=3.81\n",
      "Iter 68  time=6.79  loss=89.49    feature_norm=3.82\n",
      "Iter 69  time=6.73  loss=101.53   feature_norm=3.82\n",
      "Iter 70  time=6.78  loss=88.48    feature_norm=3.83\n",
      "Iter 71  time=6.64  loss=141.62   feature_norm=3.84\n",
      "Iter 72  time=6.55  loss=92.11    feature_norm=3.84\n",
      "Iter 73  time=6.49  loss=86.00    feature_norm=3.85\n",
      "Iter 74  time=6.44  loss=144.17   feature_norm=3.86\n",
      "Iter 75  time=7.21  loss=86.64    feature_norm=3.87\n",
      "Iter 76  time=7.02  loss=106.99   feature_norm=3.87\n",
      "Iter 77  time=6.61  loss=59.06    feature_norm=3.87\n",
      "Iter 78  time=5.77  loss=68.42    feature_norm=3.88\n",
      "Iter 79  time=6.03  loss=91.15    feature_norm=3.88\n",
      "Iter 80  time=5.77  loss=118.75   feature_norm=3.89\n",
      "Iter 81  time=6.03  loss=133.04   feature_norm=3.90\n",
      "Iter 82  time=5.89  loss=47.10    feature_norm=3.90\n",
      "Iter 83  time=5.88  loss=71.29    feature_norm=3.91\n",
      "Iter 84  time=5.66  loss=47.71    feature_norm=3.91\n",
      "Iter 85  time=5.67  loss=22.93    feature_norm=3.91\n",
      "Iter 86  time=6.22  loss=102.05   feature_norm=3.92\n",
      "Iter 87  time=5.60  loss=155.20   feature_norm=3.93\n",
      "Iter 88  time=5.71  loss=74.98    feature_norm=3.94\n",
      "Iter 89  time=6.14  loss=100.75   feature_norm=3.95\n",
      "Iter 90  time=5.74  loss=128.25   feature_norm=3.96\n",
      "Iter 91  time=5.68  loss=139.07   feature_norm=3.97\n",
      "Iter 92  time=5.68  loss=132.39   feature_norm=3.97\n",
      "Iter 93  time=5.65  loss=140.59   feature_norm=3.98\n",
      "Iter 94  time=5.58  loss=135.96   feature_norm=3.99\n",
      "Iter 95  time=5.89  loss=94.08    feature_norm=3.99\n",
      "Iter 96  time=5.78  loss=76.25    feature_norm=4.00\n",
      "Iter 97  time=5.72  loss=39.57    feature_norm=4.00\n",
      "Iter 98  time=5.76  loss=118.99   feature_norm=4.01\n",
      "Iter 99  time=6.54  loss=9.90     feature_norm=4.01\n",
      "Iter 100 time=5.96  loss=96.26    feature_norm=4.01\n",
      "Total seconds required for training: 634.868\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 71975 (2768289)\n",
      "Number of active attributes: 63979 (2760293)\n",
      "Number of active labels: 2 (2)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.174\n",
      "\n",
      "CRF model saved for topic 1250\n",
      "['B', 'B']\n",
      "['B', 'B']\n",
      "Precision: 0.9889182052828928, Recall: 0.6721664273055358, F1: 0.8003411791679775\n",
      "Topic 1250 - Avg Precision: 0.9889, Avg Recall: 0.6722, Avg F1-Score: 0.8003\n",
      "for fold: 3\n",
      "label\n",
      "1    26\n",
      "B    26\n",
      "Name: count, dtype: int64 label\n",
      "B    184062\n",
      "1        26\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    105\n",
      "B    105\n",
      "Name: count, dtype: int64 label\n",
      "B    747334\n",
      "1       105\n",
      "Name: count, dtype: int64\n",
      "32415991    Exhibit 10.1\n",
      "32415992     J.P. Morgan\n",
      "Name: sentence, dtype: object\n",
      "First sentence features in training data: [[{'token': 'Exhibit', 'lower': 'exhibit', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'E', 'prefix-2': 'Ex', 'prefix-3': 'Exh', 'suffix-1': 't', 'suffix-2': 'it', 'suffix-3': 'bit', 'prev_token': '', 'next_token': '10.1', 'is_numeric': False, 'unigram': 'Exhibit', 'bigram': 'Exhibit 10.1', 'trigram': ''}, {'token': '10.1', 'lower': '10.1', 'is_first': False, 'is_last': True, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': '1', 'prefix-2': '10', 'prefix-3': '10.', 'suffix-1': '1', 'suffix-2': '.1', 'suffix-3': '0.1', 'prev_token': 'Exhibit', 'next_token': '', 'is_numeric': False, 'unigram': '10.1', 'bigram': '', 'trigram': ''}], [{'token': 'J.P.', 'lower': 'j.p.', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'J', 'prefix-2': 'J.', 'prefix-3': 'J.P', 'suffix-1': '.', 'suffix-2': 'P.', 'suffix-3': '.P.', 'prev_token': '', 'next_token': 'Morgan', 'is_numeric': False, 'unigram': 'J.P.', 'bigram': 'J.P. Morgan', 'trigram': ''}, {'token': 'Morgan', 'lower': 'morgan', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'M', 'prefix-2': 'Mo', 'prefix-3': 'Mor', 'suffix-1': 'n', 'suffix-2': 'an', 'suffix-3': 'gan', 'prev_token': 'J.P.', 'next_token': '', 'is_numeric': False, 'unigram': 'Morgan', 'bigram': '', 'trigram': ''}]]\n",
      "First sentence labels in training data: [['B', 'B'], ['B', 'B']]\n",
      "Training model for topic 1250 for fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|█| 747439/747439 [04:05<00:00, 3048.99it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 2796712\n",
      "Seconds required: 61.890\n",
      "\n",
      "Passive Aggressive\n",
      "type: 2\n",
      "c: 0.100000\n",
      "error_sensitive: 1\n",
      "averaging: 1\n",
      "max_iterations: 100\n",
      "epsilon: 0.000000\n",
      "\n",
      "Iter 1   time=7.13  loss=3012.18  feature_norm=1.18\n",
      "Iter 2   time=6.83  loss=2126.32  feature_norm=1.62\n",
      "Iter 3   time=6.75  loss=1785.39  feature_norm=1.94\n",
      "Iter 4   time=6.29  loss=1641.70  feature_norm=2.19\n",
      "Iter 5   time=6.40  loss=1419.06  feature_norm=2.41\n",
      "Iter 6   time=6.48  loss=1108.09  feature_norm=2.52\n",
      "Iter 7   time=6.92  loss=816.65   feature_norm=2.62\n",
      "Iter 8   time=6.77  loss=767.93   feature_norm=2.72\n",
      "Iter 9   time=6.70  loss=695.41   feature_norm=2.80\n",
      "Iter 10  time=7.29  loss=580.39   feature_norm=2.84\n",
      "Iter 11  time=6.52  loss=721.76   feature_norm=2.90\n",
      "Iter 12  time=6.59  loss=514.17   feature_norm=2.95\n",
      "Iter 13  time=6.66  loss=552.63   feature_norm=3.00\n",
      "Iter 14  time=6.50  loss=626.70   feature_norm=3.06\n",
      "Iter 15  time=6.48  loss=570.08   feature_norm=3.12\n",
      "Iter 16  time=6.88  loss=379.41   feature_norm=3.15\n",
      "Iter 17  time=6.62  loss=312.42   feature_norm=3.18\n",
      "Iter 18  time=6.63  loss=420.17   feature_norm=3.22\n",
      "Iter 19  time=7.10  loss=395.80   feature_norm=3.25\n",
      "Iter 20  time=6.75  loss=323.42   feature_norm=3.28\n",
      "Iter 21  time=6.87  loss=427.81   feature_norm=3.32\n",
      "Iter 22  time=6.70  loss=420.83   feature_norm=3.37\n",
      "Iter 23  time=6.61  loss=306.74   feature_norm=3.40\n",
      "Iter 24  time=6.30  loss=250.74   feature_norm=3.41\n",
      "Iter 25  time=6.29  loss=208.48   feature_norm=3.43\n",
      "Iter 26  time=6.75  loss=561.78   feature_norm=3.48\n",
      "Iter 27  time=7.04  loss=378.97   feature_norm=3.50\n",
      "Iter 28  time=6.42  loss=320.42   feature_norm=3.52\n",
      "Iter 29  time=6.55  loss=273.74   feature_norm=3.54\n",
      "Iter 30  time=6.48  loss=271.39   feature_norm=3.55\n",
      "Iter 31  time=6.42  loss=303.12   feature_norm=3.57\n",
      "Iter 32  time=6.55  loss=266.43   feature_norm=3.59\n",
      "Iter 33  time=6.45  loss=208.22   feature_norm=3.60\n",
      "Iter 34  time=6.49  loss=276.69   feature_norm=3.62\n",
      "Iter 35  time=6.41  loss=310.04   feature_norm=3.64\n",
      "Iter 36  time=6.89  loss=218.24   feature_norm=3.66\n",
      "Iter 37  time=6.55  loss=208.92   feature_norm=3.67\n",
      "Iter 38  time=6.53  loss=243.79   feature_norm=3.69\n",
      "Iter 39  time=6.41  loss=119.51   feature_norm=3.70\n",
      "Iter 40  time=6.57  loss=100.89   feature_norm=3.72\n",
      "Iter 41  time=6.54  loss=148.53   feature_norm=3.73\n",
      "Iter 42  time=6.48  loss=151.54   feature_norm=3.75\n",
      "Iter 43  time=6.53  loss=275.27   feature_norm=3.77\n",
      "Iter 44  time=6.56  loss=241.31   feature_norm=3.79\n",
      "Iter 45  time=6.77  loss=234.23   feature_norm=3.81\n",
      "Iter 46  time=6.37  loss=143.34   feature_norm=3.82\n",
      "Iter 47  time=6.36  loss=165.73   feature_norm=3.83\n",
      "Iter 48  time=6.93  loss=218.02   feature_norm=3.83\n",
      "Iter 49  time=6.69  loss=236.71   feature_norm=3.86\n",
      "Iter 50  time=6.36  loss=189.34   feature_norm=3.88\n",
      "Iter 51  time=6.35  loss=166.43   feature_norm=3.88\n",
      "Iter 52  time=6.37  loss=112.25   feature_norm=3.90\n",
      "Iter 53  time=6.58  loss=245.77   feature_norm=3.92\n",
      "Iter 54  time=7.04  loss=177.66   feature_norm=3.94\n",
      "Iter 55  time=6.65  loss=197.80   feature_norm=3.95\n",
      "Iter 56  time=6.62  loss=234.42   feature_norm=3.95\n",
      "Iter 57  time=6.55  loss=150.87   feature_norm=3.96\n",
      "Iter 58  time=6.38  loss=99.22    feature_norm=3.96\n",
      "Iter 59  time=6.50  loss=69.56    feature_norm=3.97\n",
      "Iter 60  time=6.39  loss=129.18   feature_norm=3.97\n",
      "Iter 61  time=6.39  loss=115.28   feature_norm=3.98\n",
      "Iter 62  time=7.01  loss=96.88    feature_norm=3.98\n",
      "Iter 63  time=6.48  loss=113.02   feature_norm=3.99\n",
      "Iter 64  time=6.48  loss=116.19   feature_norm=3.99\n",
      "Iter 65  time=6.44  loss=137.92   feature_norm=4.00\n",
      "Iter 66  time=6.42  loss=168.75   feature_norm=4.01\n",
      "Iter 67  time=6.39  loss=69.21    feature_norm=4.01\n",
      "Iter 68  time=6.42  loss=122.98   feature_norm=4.02\n",
      "Iter 69  time=6.60  loss=207.24   feature_norm=4.04\n",
      "Iter 70  time=6.38  loss=130.03   feature_norm=4.05\n",
      "Iter 71  time=6.51  loss=57.95    feature_norm=4.06\n",
      "Iter 72  time=6.42  loss=127.13   feature_norm=4.06\n",
      "Iter 73  time=6.47  loss=58.73    feature_norm=4.07\n",
      "Iter 74  time=6.41  loss=86.13    feature_norm=4.08\n",
      "Iter 75  time=6.41  loss=202.46   feature_norm=4.09\n",
      "Iter 76  time=6.65  loss=109.41   feature_norm=4.10\n",
      "Iter 77  time=6.42  loss=55.80    feature_norm=4.09\n",
      "Iter 78  time=6.54  loss=218.34   feature_norm=4.10\n",
      "Iter 79  time=6.42  loss=151.44   feature_norm=4.12\n",
      "Iter 80  time=7.00  loss=151.49   feature_norm=4.13\n",
      "Iter 81  time=6.37  loss=99.73    feature_norm=4.14\n",
      "Iter 82  time=6.42  loss=89.47    feature_norm=4.14\n",
      "Iter 83  time=6.38  loss=79.49    feature_norm=4.15\n",
      "Iter 84  time=6.50  loss=112.40   feature_norm=4.15\n",
      "Iter 85  time=6.48  loss=139.34   feature_norm=4.16\n",
      "Iter 86  time=6.44  loss=159.50   feature_norm=4.16\n",
      "Iter 87  time=7.26  loss=118.59   feature_norm=4.17\n",
      "Iter 88  time=6.69  loss=128.97   feature_norm=4.19\n",
      "Iter 89  time=6.73  loss=88.24    feature_norm=4.19\n",
      "Iter 90  time=6.34  loss=151.45   feature_norm=4.20\n",
      "Iter 91  time=6.43  loss=137.29   feature_norm=4.21\n",
      "Iter 92  time=9.55  loss=61.44    feature_norm=4.21\n",
      "Iter 93  time=7.58  loss=57.75    feature_norm=4.21\n",
      "Iter 94  time=6.57  loss=116.49   feature_norm=4.22\n",
      "Iter 95  time=6.61  loss=123.47   feature_norm=4.23\n",
      "Iter 96  time=6.92  loss=32.41    feature_norm=4.23\n",
      "Iter 97  time=6.73  loss=115.05   feature_norm=4.23\n",
      "Iter 98  time=6.69  loss=90.90    feature_norm=4.24\n",
      "Iter 99  time=6.73  loss=93.43    feature_norm=4.25\n",
      "Iter 100 time=6.61  loss=74.46    feature_norm=4.25\n",
      "Total seconds required for training: 662.831\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 73965 (2796712)\n",
      "Number of active attributes: 65586 (2788333)\n",
      "Number of active labels: 2 (2)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.181\n",
      "\n",
      "CRF model saved for topic 1250\n",
      "['B', 'B']\n",
      "['B', 'B']\n",
      "Precision: 0.9999999994044073, Recall: 0.7525773192503016, F1: 0.8588230389381464\n",
      "Topic 1250 - Avg Precision: 1.0000, Avg Recall: 0.7526, Avg F1-Score: 0.8588\n",
      "for fold: 4\n",
      "label\n",
      "1    21\n",
      "B    21\n",
      "Name: count, dtype: int64 label\n",
      "B    170880\n",
      "1        21\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    110\n",
      "B    110\n",
      "Name: count, dtype: int64 label\n",
      "B    760516\n",
      "1       110\n",
      "Name: count, dtype: int64\n",
      "32413173          EXECUTION VERSION\n",
      "32413174    THE BANK OF NOVA SCOTIA\n",
      "Name: sentence, dtype: object\n",
      "First sentence features in training data: [[{'token': 'EXECUTION', 'lower': 'execution', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'E', 'prefix-2': 'EX', 'prefix-3': 'EXE', 'suffix-1': 'N', 'suffix-2': 'ON', 'suffix-3': 'ION', 'prev_token': '', 'next_token': 'VERSION', 'is_numeric': False, 'unigram': 'EXECUTION', 'bigram': 'EXECUTION VERSION', 'trigram': ''}, {'token': 'VERSION', 'lower': 'version', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'V', 'prefix-2': 'VE', 'prefix-3': 'VER', 'suffix-1': 'N', 'suffix-2': 'ON', 'suffix-3': 'ION', 'prev_token': 'EXECUTION', 'next_token': '', 'is_numeric': False, 'unigram': 'VERSION', 'bigram': '', 'trigram': ''}], [{'token': 'THE', 'lower': 'the', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'T', 'prefix-2': 'TH', 'prefix-3': 'THE', 'suffix-1': 'E', 'suffix-2': 'HE', 'suffix-3': 'THE', 'prev_token': '', 'next_token': 'BANK', 'is_numeric': False, 'unigram': 'THE', 'bigram': 'THE BANK', 'trigram': 'THE BANK OF'}, {'token': 'BANK', 'lower': 'bank', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'B', 'prefix-2': 'BA', 'prefix-3': 'BAN', 'suffix-1': 'K', 'suffix-2': 'NK', 'suffix-3': 'ANK', 'prev_token': 'THE', 'next_token': 'OF', 'is_numeric': False, 'unigram': 'BANK', 'bigram': 'BANK OF', 'trigram': 'BANK OF NOVA'}, {'token': 'OF', 'lower': 'of', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'O', 'prefix-2': 'OF', 'prefix-3': 'OF', 'suffix-1': 'F', 'suffix-2': 'OF', 'suffix-3': 'OF', 'prev_token': 'BANK', 'next_token': 'NOVA', 'is_numeric': False, 'unigram': 'OF', 'bigram': 'OF NOVA', 'trigram': 'OF NOVA SCOTIA'}, {'token': 'NOVA', 'lower': 'nova', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'N', 'prefix-2': 'NO', 'prefix-3': 'NOV', 'suffix-1': 'A', 'suffix-2': 'VA', 'suffix-3': 'OVA', 'prev_token': 'OF', 'next_token': 'SCOTIA', 'is_numeric': False, 'unigram': 'NOVA', 'bigram': 'NOVA SCOTIA', 'trigram': ''}, {'token': 'SCOTIA', 'lower': 'scotia', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'S', 'prefix-2': 'SC', 'prefix-3': 'SCO', 'suffix-1': 'A', 'suffix-2': 'IA', 'suffix-3': 'TIA', 'prev_token': 'NOVA', 'next_token': '', 'is_numeric': False, 'unigram': 'SCOTIA', 'bigram': '', 'trigram': ''}]]\n",
      "First sentence labels in training data: [['B', 'B'], ['B', 'B', 'B', 'B', 'B']]\n",
      "Training model for topic 1250 for fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|█| 760626/760626 [04:31<00:00, 2796.48it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 2854799\n",
      "Seconds required: 65.810\n",
      "\n",
      "Passive Aggressive\n",
      "type: 2\n",
      "c: 0.100000\n",
      "error_sensitive: 1\n",
      "averaging: 1\n",
      "max_iterations: 100\n",
      "epsilon: 0.000000\n",
      "\n",
      "Iter 1   time=7.86  loss=3106.78  feature_norm=1.18\n",
      "Iter 2   time=7.47  loss=2394.60  feature_norm=1.64\n",
      "Iter 3   time=7.15  loss=1844.72  feature_norm=1.94\n",
      "Iter 4   time=7.62  loss=1666.57  feature_norm=2.13\n",
      "Iter 5   time=7.22  loss=1261.67  feature_norm=2.32\n",
      "Iter 6   time=7.15  loss=1150.85  feature_norm=2.48\n",
      "Iter 7   time=7.05  loss=932.07   feature_norm=2.60\n",
      "Iter 8   time=6.99  loss=981.41   feature_norm=2.69\n",
      "Iter 9   time=6.96  loss=567.79   feature_norm=2.75\n",
      "Iter 10  time=7.11  loss=694.05   feature_norm=2.82\n",
      "Iter 11  time=10.29 loss=411.88   feature_norm=2.85\n",
      "Iter 12  time=8.21  loss=439.27   feature_norm=2.89\n",
      "Iter 13  time=7.05  loss=529.47   feature_norm=2.94\n",
      "Iter 14  time=7.06  loss=662.78   feature_norm=3.01\n",
      "Iter 15  time=6.97  loss=558.68   feature_norm=3.06\n",
      "Iter 16  time=7.02  loss=231.77   feature_norm=3.07\n",
      "Iter 17  time=7.33  loss=559.75   feature_norm=3.12\n",
      "Iter 18  time=7.19  loss=280.43   feature_norm=3.15\n",
      "Iter 19  time=7.20  loss=474.88   feature_norm=3.19\n",
      "Iter 20  time=7.17  loss=186.67   feature_norm=3.22\n",
      "Iter 21  time=7.41  loss=250.01   feature_norm=3.23\n",
      "Iter 22  time=7.18  loss=293.64   feature_norm=3.26\n",
      "Iter 23  time=7.38  loss=289.67   feature_norm=3.29\n",
      "Iter 24  time=7.01  loss=407.37   feature_norm=3.33\n",
      "Iter 25  time=7.23  loss=312.15   feature_norm=3.34\n",
      "Iter 26  time=7.24  loss=256.62   feature_norm=3.36\n",
      "Iter 27  time=7.56  loss=378.32   feature_norm=3.39\n",
      "Iter 28  time=7.25  loss=211.30   feature_norm=3.42\n",
      "Iter 29  time=7.66  loss=106.81   feature_norm=3.42\n",
      "Iter 30  time=7.16  loss=212.07   feature_norm=3.44\n",
      "Iter 31  time=7.28  loss=267.25   feature_norm=3.46\n",
      "Iter 32  time=7.19  loss=163.83   feature_norm=3.47\n",
      "Iter 33  time=7.14  loss=91.60    feature_norm=3.47\n",
      "Iter 34  time=7.46  loss=262.39   feature_norm=3.49\n",
      "Iter 35  time=7.52  loss=196.45   feature_norm=3.49\n",
      "Iter 36  time=7.20  loss=24.69    feature_norm=3.50\n",
      "Iter 37  time=7.12  loss=30.31    feature_norm=3.50\n",
      "Iter 38  time=7.04  loss=156.07   feature_norm=3.51\n",
      "Iter 39  time=7.13  loss=176.71   feature_norm=3.52\n",
      "Iter 40  time=7.08  loss=59.52    feature_norm=3.52\n",
      "Iter 41  time=7.16  loss=207.31   feature_norm=3.54\n",
      "Iter 42  time=7.33  loss=205.16   feature_norm=3.56\n",
      "Iter 43  time=7.41  loss=54.32    feature_norm=3.56\n",
      "Iter 44  time=7.28  loss=188.32   feature_norm=3.58\n",
      "Iter 45  time=7.24  loss=130.32   feature_norm=3.59\n",
      "Iter 46  time=7.19  loss=128.94   feature_norm=3.60\n",
      "Iter 47  time=7.08  loss=197.75   feature_norm=3.61\n",
      "Iter 48  time=7.08  loss=175.60   feature_norm=3.62\n",
      "Iter 49  time=7.12  loss=64.97    feature_norm=3.62\n",
      "Iter 50  time=7.35  loss=69.35    feature_norm=3.63\n",
      "Iter 51  time=7.05  loss=87.24    feature_norm=3.64\n",
      "Iter 52  time=7.12  loss=121.56   feature_norm=3.65\n",
      "Iter 53  time=7.13  loss=153.72   feature_norm=3.66\n",
      "Iter 54  time=7.39  loss=121.09   feature_norm=3.67\n",
      "Iter 55  time=7.11  loss=151.15   feature_norm=3.68\n",
      "Iter 56  time=6.94  loss=151.48   feature_norm=3.69\n",
      "Iter 57  time=6.99  loss=166.98   feature_norm=3.71\n",
      "Iter 58  time=7.61  loss=222.89   feature_norm=3.73\n",
      "Iter 59  time=7.04  loss=35.24    feature_norm=3.73\n",
      "Iter 60  time=7.38  loss=95.49    feature_norm=3.74\n",
      "Iter 61  time=7.21  loss=215.16   feature_norm=3.75\n",
      "Iter 62  time=7.44  loss=108.56   feature_norm=3.76\n",
      "Iter 63  time=7.43  loss=106.90   feature_norm=3.77\n",
      "Iter 64  time=7.07  loss=58.90    feature_norm=3.77\n",
      "Iter 65  time=7.13  loss=77.89    feature_norm=3.78\n",
      "Iter 66  time=7.51  loss=179.32   feature_norm=3.79\n",
      "Iter 67  time=7.37  loss=85.50    feature_norm=3.79\n",
      "Iter 68  time=7.13  loss=25.96    feature_norm=3.79\n",
      "Iter 69  time=7.12  loss=60.56    feature_norm=3.79\n",
      "Iter 70  time=7.13  loss=208.35   feature_norm=3.81\n",
      "Iter 71  time=7.47  loss=147.96   feature_norm=3.82\n",
      "Iter 72  time=7.01  loss=183.17   feature_norm=3.84\n",
      "Iter 73  time=7.24  loss=73.38    feature_norm=3.85\n",
      "Iter 74  time=7.38  loss=29.26    feature_norm=3.85\n",
      "Iter 75  time=7.29  loss=67.57    feature_norm=3.86\n",
      "Iter 76  time=7.42  loss=279.81   feature_norm=3.87\n",
      "Iter 77  time=7.14  loss=42.76    feature_norm=3.88\n",
      "Iter 78  time=7.18  loss=87.75    feature_norm=3.89\n",
      "Iter 79  time=7.06  loss=75.98    feature_norm=3.89\n",
      "Iter 80  time=7.04  loss=34.34    feature_norm=3.89\n",
      "Iter 81  time=7.25  loss=98.21    feature_norm=3.89\n",
      "Iter 82  time=7.29  loss=124.69   feature_norm=3.90\n",
      "Iter 83  time=7.24  loss=55.73    feature_norm=3.91\n",
      "Iter 84  time=7.14  loss=96.99    feature_norm=3.92\n",
      "Iter 85  time=7.11  loss=38.50    feature_norm=3.91\n",
      "Iter 86  time=6.96  loss=28.12    feature_norm=3.91\n",
      "Iter 87  time=7.20  loss=22.18    feature_norm=3.92\n",
      "Iter 88  time=6.99  loss=61.95    feature_norm=3.92\n",
      "Iter 89  time=7.32  loss=25.90    feature_norm=3.92\n",
      "Iter 90  time=7.32  loss=66.86    feature_norm=3.92\n",
      "Iter 91  time=7.13  loss=44.52    feature_norm=3.93\n",
      "Iter 92  time=7.23  loss=49.61    feature_norm=3.93\n",
      "Iter 93  time=7.20  loss=23.19    feature_norm=3.93\n",
      "Iter 94  time=7.37  loss=161.77   feature_norm=3.94\n",
      "Iter 95  time=7.42  loss=130.53   feature_norm=3.95\n",
      "Iter 96  time=7.34  loss=113.56   feature_norm=3.96\n",
      "Iter 97  time=7.32  loss=47.33    feature_norm=3.96\n",
      "Iter 98  time=7.20  loss=98.24    feature_norm=3.97\n",
      "Iter 99  time=7.28  loss=84.60    feature_norm=3.97\n",
      "Iter 100 time=7.52  loss=16.28    feature_norm=3.97\n",
      "Total seconds required for training: 726.958\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 75624 (2854799)\n",
      "Number of active attributes: 66947 (2846122)\n",
      "Number of active labels: 2 (2)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.183\n",
      "\n",
      "CRF model saved for topic 1250\n",
      "['B', 'B', 'B', 'B']\n",
      "['B', 'B', 'B', 'B']\n",
      "Precision: 0.9351432873792282, Recall: 0.7612031916751361, F1: 0.8392550038432719\n",
      "Topic 1250 - Avg Precision: 0.9351, Avg Recall: 0.7612, Avg F1-Score: 0.8393\n",
      "Saving the prediction\n",
      "Precision: 0.9789322479077169, Recall: 0.7715094338894802, F1: 0.8629308143057968\n",
      "Sentence level results\n",
      "Topic 1250 - Avg Precision: 0.9789, Avg Recall: 0.7715, Avg F1-Score: 0.8629\n",
      "81\n",
      "66\n",
      "{'TP': 63, 'FP': 6, 'FN': 3, 'Precision': 0.9130434650283556, 'Recall': 0.9545454400826449, 'F1-Score': 0.9333328197533544}\n",
      "Results saved to raw_data_exp_25_04_24/1250/results.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store predictions and labels\n",
    "\n",
    "# Iterate over folders\n",
    "for folder in folders_to_process:\n",
    "\n",
    "    if folder in os.listdir(path):  # Check if the folder is actually in the directory\n",
    "        for fold in range(5):\n",
    "            \n",
    "            ####### Data split into train and test sets #######\n",
    "            print(\"for fold:\", fold)\n",
    "            test_split = fold\n",
    "            train_split = [i for i in range(5) if i != test_split]\n",
    "            \n",
    "            test = read_split(f'{path}/{folder}/{folder}-{test_split}.cache')\n",
    "            train = sum([read_split(f'{path}/{folder}/{folder}-{el}.cache') for el in train_split], [])\n",
    "\n",
    "            df_ = df[df['topic_id'] == int(folder)]\n",
    "            df_ = df_.dropna()\n",
    "            df_['doc_id'] = df_.apply(map_doc, axis=1)\n",
    "\n",
    "            df_test = df_[df_['doc_id'].isin(test)]\n",
    "            df_train = df_[df_['doc_id'].isin(train)]\n",
    "\n",
    "            test_unbalanced, test_balanced = stratify(df_test)\n",
    "            train_unbalanced, train_balanced = stratify(df_train)\n",
    "            \n",
    "            print(test_balanced['label'].value_counts(), test_unbalanced['label'].value_counts())\n",
    "            print(train_balanced['label'].value_counts(), train_unbalanced['label'].value_counts())\n",
    "            \n",
    "            ######### Model training ##############\n",
    "\n",
    "            model_save_dir = 'raw_data_exp_25_04_24'\n",
    "            os.makedirs(model_save_dir, exist_ok=True)\n",
    "            \n",
    "            ## train CRF model\n",
    "            train_crf_model(train_unbalanced, test_unbalanced, model_save_dir, str(int(folder)), fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f4548",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea9c6e",
   "metadata": {},
   "source": [
    "### Sentence level eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ba401d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_level_results(true_labels_flat, predicted_labels_flat):\n",
    "    \n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for gold, pred in zip(true_labels_flat, predicted_labels_flat):\n",
    "        # Skip empty strings\n",
    "        if gold == '' or pred == '':\n",
    "            continue\n",
    "        gold = 0 if gold == 'B' else int(gold)\n",
    "        pred = 0 if pred == 'B' else int(pred)\n",
    "\n",
    "        if gold == 1 and pred == 1:\n",
    "            tp += 1\n",
    "        elif gold != 1 and pred == 1:\n",
    "            fp += 1\n",
    "        elif gold == 1 and pred != 1:\n",
    "            fn += 1\n",
    "\n",
    "    # Compute metrics\n",
    "    eps = 1e-6\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall = tp / (tp + fn + eps)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + eps)\n",
    "\n",
    "    print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b5ed42",
   "metadata": {},
   "source": [
    "### Annotation level eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7934d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_spans(labels):\n",
    "    \"\"\"\n",
    "    Converts a list of labels ('1' or 'B') into spans.\n",
    "    \n",
    "    Args:\n",
    "    - labels: List of labels ('1' or 'B') for tokens.\n",
    "\n",
    "    Returns:\n",
    "    - A list of spans represented as tuples (start, end).\n",
    "    \"\"\"\n",
    "    start = None\n",
    "    spans = []\n",
    "    for pos, label in enumerate(labels):\n",
    "        if label == \"1\" and start is None:  # Start of a new span\n",
    "            start = pos\n",
    "        elif label != \"1\" and start is not None:  # End of the current span\n",
    "            spans.append((start, pos - 1))\n",
    "            start = None\n",
    "    if start is not None:  # If a span extends to the end of the sequence\n",
    "        spans.append((start, len(labels) - 1))\n",
    "        \n",
    "    print(len(spans))\n",
    "        \n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab881aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels_and_create_spans(file_path, output_file):\n",
    "    with open(file_path, 'r') as f:\n",
    "        raw_data = f.readlines()\n",
    "        \n",
    "    # Initialize variables\n",
    "    current_sequence = []\n",
    "    all_spans = []\n",
    "    \n",
    "    for line in raw_data:\n",
    "        label = line.strip()\n",
    "        if label:  # If the line is not empty, add the label to the current sequence\n",
    "            current_sequence.append(label)\n",
    "        else:  # If the line is empty, process the current sequence and reset it\n",
    "            if current_sequence:  # Check if the current sequence is not empty\n",
    "                spans = labels_to_spans(current_sequence)\n",
    "                all_spans.extend(spans)\n",
    "                current_sequence = []  # Reset the sequence for the next block\n",
    "\n",
    "    # Process the last sequence if the file doesn't end with a blank line\n",
    "    if current_sequence:\n",
    "        spans = labels_to_spans(current_sequence)\n",
    "        all_spans.extend(spans)\n",
    "    \n",
    "    # Write the spans to the output file\n",
    "    with open(output_file, 'w') as f:\n",
    "        for span in all_spans:\n",
    "            f.write(f\"{span[0]} {span[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aa66235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_span_file(filename):\n",
    "    \"\"\"Parses a span file to extract spans.\"\"\"\n",
    "    spans = []\n",
    "    with open(filename) as fil:\n",
    "        for line in fil:\n",
    "            start, end = map(int, line.strip().split())\n",
    "            spans.append({'start': start, 'end': 1 + end})\n",
    "    return spans\n",
    "\n",
    "def span_overlaps(A, B):\n",
    "    \"\"\"Checks if two spans overlap.\"\"\"\n",
    "    return not ((A['end'] <= B['start']) or (A['start'] >= B['end']))\n",
    "\n",
    "def overlapping_spans(A, B):\n",
    "    \"\"\"Finds overlapping spans between two lists of spans.\"\"\"\n",
    "    return [a for a in A if any(span_overlaps(a, b) for b in B)]\n",
    "\n",
    "def non_overlapping_spans(A, B):\n",
    "    \"\"\"Finds spans in A that do not overlap with any span in B.\"\"\"\n",
    "    return [a for a in A if not any(span_overlaps(a, b) for b in B)]\n",
    "\n",
    "def evaluate_annotations(gold_file, pred_file):\n",
    "    \"\"\"Evaluates the predicted annotations against the gold standard.\"\"\"\n",
    "    gold = parse_span_file(gold_file)\n",
    "    pred = parse_span_file(pred_file)\n",
    "    \n",
    "    eps = 0.000001\n",
    "    tp = len(overlapping_spans(gold, pred))\n",
    "    fp = len(non_overlapping_spans(pred, gold))\n",
    "    fn = len(non_overlapping_spans(gold, pred))\n",
    "    \n",
    "    recall = tp / float(tp + fn + eps)\n",
    "    precision = tp / float(tp + fp + eps)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + eps)\n",
    "    \n",
    "    return {\"TP\": tp, \"FP\": fp, \"FN\": fn, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from itertools import chain\n",
    "import os\n",
    "import sklearn_crfsuite\n",
    "\n",
    "def load_crf_model(model_path):\n",
    "    \"\"\"Load a saved CRF model from a file.\"\"\"\n",
    "    with open(model_path, 'rb') as model_file:\n",
    "        crf_model = pickle.load(model_file)\n",
    "    return crf_model\n",
    "\n",
    "def predict_with_crf(crf_model, X_test):\n",
    "    \"\"\"Predict using a loaded CRF model.\"\"\"\n",
    "    return crf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b5be3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_predictions(y_test, y_pred, fold, results_df, model_save_dir, folder):\n",
    "    \"\"\"Evaluate the model predictions using appropriate metrics.\"\"\"\n",
    "    \n",
    "\n",
    "    topic_id = folder\n",
    "    y_test_flat = list(chain.from_iterable(y_test))\n",
    "    y_pred_flat = list(chain.from_iterable(y_pred))\n",
    "    \n",
    "    # Save the combined predictions and gold labels for the topic\n",
    "    pred_file_path = os.path.join(model_save_dir, topic_id, f\"{topic_id}_{fold}.pred.raw\")\n",
    "    gold_file_path = os.path.join(model_save_dir, topic_id, f\"{topic_id}_{fold}.gold.raw\")\n",
    "\n",
    "    with open(pred_file_path, \"w\") as pred_file, open(gold_file_path, \"w\") as gold_file:\n",
    "        pred_file.write(\"\\n\".join(y_test_flat))\n",
    "        gold_file.write(\"\\n\".join(y_pred_flat))\n",
    "    \n",
    "    precision, recall, f1_score = sentence_level_results(y_test_flat, y_pred_flat)\n",
    "    print(f\"Avg Precision: {precision:.4f}, Avg Recall: {recall:.4f}, Avg F1-Score: {f1_score:.4f}\")\n",
    "    \n",
    "    # Create spans and save to .span files\n",
    "    load_labels_and_create_spans(pred_file_path, os.path.join(model_save_dir, topic_id, f\"{topic_id}_{fold}.pred.span\"))\n",
    "    load_labels_and_create_spans(gold_file_path, os.path.join(model_save_dir, topic_id, f\"{topic_id}_{fold}.gold.span\"))\n",
    "\n",
    "    # Evaluate the annotations\n",
    "    metrics = evaluate_annotations(os.path.join(model_save_dir, topic_id, f\"{topic_id}_{fold}.gold.span\"), os.path.join(model_save_dir, topic_id, f\"{topic_id}_{fold}.pred.span\"))\n",
    "    print(metrics)\n",
    "    \n",
    "    new_row = pd.DataFrame([{\n",
    "        \"Topic ID\": topic_id,\n",
    "        \"Fold\": fold,\n",
    "        \"Sentence Precision\": precision,\n",
    "        \"Sentence Recall\": recall,\n",
    "        \"Sentence F1-Score\": f1_score,\n",
    "        \"Annotation TP\": metrics[\"TP\"],\n",
    "        \"Annotation FP\": metrics[\"FP\"],\n",
    "        \"Annotation FN\": metrics[\"FN\"],\n",
    "        \"Annotation Precision\": metrics[\"Precision\"],\n",
    "        \"Annotation Recall\": metrics[\"Recall\"],\n",
    "        \"Annotation F1-Score\": metrics[\"F1-Score\"],\n",
    "    }], columns=results_df.columns)\n",
    "\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "77a5d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_crf_model(model_save_dir, folders_to_process, path, df):\n",
    "    for folder in folders_to_process:\n",
    "        \n",
    "        if folder in os.listdir(path):\n",
    "            \n",
    "            ## Dataframe to store the results\n",
    "            results_df = pd.DataFrame(columns=[\"Topic ID\", \"Fold\", \"Sentence Precision\", \"Sentence Recall\", \"Sentence F1-Score\",\n",
    "                                       \"Annotation TP\", \"Annotation FP\", \"Annotation FN\", \n",
    "                                       \"Annotation Precision\", \"Annotation Recall\", \"Annotation F1-Score\"])\n",
    "            for fold in range(5):\n",
    "                print(f\"Processing topic {folder}, fold {fold}\")\n",
    "                ### Load the saved model for the topic and fold\n",
    "                fold_model_path = os.path.join(model_save_dir, folder, f\"{folder}_crf_model_{fold}.pkl\")\n",
    "                if os.path.exists(fold_model_path):\n",
    "                    crf_model = load_crf_model(fold_model_path)\n",
    "                    \n",
    "                    # Load test data splits\n",
    "                    test = read_split(f'{path}/{folder}/{folder}-{fold}.cache')\n",
    "                    \n",
    "                    ## filter data for topic\n",
    "                    df_ = df[df['topic_id'] == int(folder)]\n",
    "                    df_ = df_.dropna()\n",
    "                    df_['doc_id'] = df_.apply(map_doc, axis=1)\n",
    "\n",
    "                    df_test = df_[df_['doc_id'].isin(test)]\n",
    "                    \n",
    "                    ## genertae test data\n",
    "                    test_unbalanced, test_balanced = stratify(df_test)\n",
    "\n",
    "                    # Extract features and predict\n",
    "                    test_sentences = test_unbalanced['sentence']\n",
    "                    test_labels = test_unbalanced['label']\n",
    "                    \n",
    "                    test_extracted = [process_text_and_extract_features(sentence, label) for sentence, label in zip(test_sentences, test_labels)]\n",
    "                    X_test = [features for features, _ in test_extracted]\n",
    "                    y_test = [labels for _, labels in test_extracted]\n",
    "                    \n",
    "                    ## Predict\n",
    "                    y_pred = predict_with_crf(crf_model, X_test)\n",
    "                    results_df = evaluate_model_predictions(y_test, y_pred, fold, results_df, model_save_dir, folder)\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"Model file not found: {fold_model_path}\")\n",
    "                    \n",
    "            print(\"\\n Completed all folds\")\n",
    "            # Save results to CSV\n",
    "            results_csv_path = os.path.join(model_save_dir, folder, \"final_results.csv\")\n",
    "            results_df.to_csv(results_csv_path, index=False)\n",
    "            print(f\"Results saved to {results_csv_path}\")\n",
    "            \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9288ac6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder 1272, fold 0\n",
      "Precision: 0.9665529003641277, Recall: 0.849939975480228, F1: 0.9045028550442747\n",
      "Avg Precision: 0.9666, Avg Recall: 0.8499, Avg F1-Score: 0.9045\n",
      "37\n",
      "40\n",
      "{'TP': 37, 'FP': 3, 'FN': 3, 'Precision': 0.9249999768750007, 'Recall': 0.9249999768750007, 'F1-Score': 0.924999476875271}\n",
      "Processing folder 1272, fold 1\n",
      "Precision: 0.9908561920275508, Recall: 0.7281612701721678, F1: 0.839436130803944\n",
      "Avg Precision: 0.9909, Avg Recall: 0.7282, Avg F1-Score: 0.8394\n",
      "30\n",
      "33\n",
      "{'TP': 31, 'FP': 1, 'FN': 2, 'Precision': 0.9687499697265636, 'Recall': 0.9393939109274573, 'F1-Score': 0.9538456246156477}\n",
      "Processing folder 1272, fold 2\n",
      "Precision: 0.9364055290908704, Recall: 0.6011834315969329, F1: 0.7322517754907407\n",
      "Avg Precision: 0.9364, Avg Recall: 0.6012, Avg F1-Score: 0.7323\n",
      "32\n",
      "35\n",
      "{'TP': 29, 'FP': 5, 'FN': 6, 'Precision': 0.8529411513840839, 'Recall': 0.8285714048979599, 'F1-Score': 0.8405791858856164}\n",
      "Processing folder 1272, fold 3\n",
      "Precision: 0.8986486478896548, Recall: 0.8377952749308698, F1: 0.867155164129308\n",
      "Avg Precision: 0.8986, Avg Recall: 0.8378, Avg F1-Score: 0.8672\n",
      "32\n",
      "40\n",
      "{'TP': 32, 'FP': 1, 'FN': 8, 'Precision': 0.969696940312214, 'Recall': 0.7999999800000005, 'F1-Score': 0.8767118093453732}\n",
      "Processing folder 1272, fold 4\n",
      "Precision: 0.9346026482329447, Recall: 0.6477337919404854, F1: 0.7651638669830042\n",
      "Avg Precision: 0.9346, Avg Recall: 0.6477, Avg F1-Score: 0.7652\n",
      "30\n",
      "37\n",
      "{'TP': 31, 'FP': 0, 'FN': 6, 'Precision': 0.9999999677419364, 'Recall': 0.8378378151935726, 'F1-Score': 0.9117641829587481}\n",
      "Results saved to raw_data_exp_25_04_24/1272/final_results.csv\n",
      "Processing folder 1474, fold 0\n",
      "Precision: 0.8830749348300548, Recall: 0.8417487679545881, F1: 0.861916271496706\n",
      "Avg Precision: 0.8831, Avg Recall: 0.8417, Avg F1-Score: 0.8619\n",
      "30\n",
      "35\n",
      "{'TP': 31, 'FP': 1, 'FN': 4, 'Precision': 0.9687499697265636, 'Recall': 0.885714260408164, 'F1-Score': 0.925372607708}\n",
      "Processing folder 1474, fold 1\n",
      "Precision: 0.979348688658182, Recall: 0.6834811526144783, F1: 0.8050925613374689\n",
      "Avg Precision: 0.9793, Avg Recall: 0.6835, Avg F1-Score: 0.8051\n",
      "32\n",
      "32\n",
      "{'TP': 30, 'FP': 3, 'FN': 2, 'Precision': 0.9090908815427007, 'Recall': 0.937499970703126, 'F1-Score': 0.923076394793171}\n",
      "Processing folder 1474, fold 2\n",
      "Precision: 0.9081115330113361, Recall: 0.9174135717558171, F1: 0.9127383529350784\n",
      "Avg Precision: 0.9081, Avg Recall: 0.9174, Avg F1-Score: 0.9127\n",
      "35\n",
      "40\n",
      "{'TP': 36, 'FP': 0, 'FN': 4, 'Precision': 0.999999972222223, 'Recall': 0.8999999775000006, 'F1-Score': 0.9473678975071883}\n",
      "Processing folder 1474, fold 3\n",
      "Precision: 0.858882124593867, Recall: 0.8277333328918756, F1: 0.8430195974858693\n",
      "Avg Precision: 0.8589, Avg Recall: 0.8277, Avg F1-Score: 0.8430\n",
      "33\n",
      "44\n",
      "{'TP': 35, 'FP': 0, 'FN': 9, 'Precision': 0.9999999714285723, 'Recall': 0.7954545273760335, 'F1-Score': 0.8860754334244059}\n",
      "Processing folder 1474, fold 4\n",
      "Precision: 0.8811333787967288, Recall: 0.788985148026618, F1: 0.8325166410375904\n",
      "Avg Precision: 0.8811, Avg Recall: 0.7890, Avg F1-Score: 0.8325\n",
      "31\n",
      "38\n",
      "{'TP': 31, 'FP': 0, 'FN': 7, 'Precision': 0.9999999677419364, 'Recall': 0.8157894522160671, 'F1-Score': 0.8985502037389834}\n",
      "Results saved to raw_data_exp_25_04_24/1474/final_results.csv\n",
      "Processing folder 1238, fold 0\n",
      "Precision: 0.8501026690553993, Recall: 0.8961038957159724, F1: 0.8724968656275729\n",
      "Avg Precision: 0.8501, Avg Recall: 0.8961, Avg F1-Score: 0.8725\n",
      "14\n",
      "19\n",
      "{'TP': 15, 'FP': 0, 'FN': 4, 'Precision': 0.9999999333333378, 'Recall': 0.7894736426592819, 'F1-Score': 0.8823523961940494}\n",
      "Processing folder 1238, fold 1\n",
      "Precision: 0.9079526223661751, Recall: 0.822753756264105, F1: 0.8632556141903367\n",
      "Avg Precision: 0.9080, Avg Recall: 0.8228, Avg F1-Score: 0.8633\n",
      "18\n",
      "21\n",
      "{'TP': 18, 'FP': 0, 'FN': 3, 'Precision': 0.9999999444444475, 'Recall': 0.8571428163265326, 'F1-Score': 0.9230763786984947}\n",
      "Processing folder 1238, fold 2\n",
      "Precision: 0.9999999995327103, Recall: 0.6850192059266904, F1: 0.8130694259773885\n",
      "Avg Precision: 1.0000, Avg Recall: 0.6850, Avg F1-Score: 0.8131\n",
      "17\n",
      "16\n",
      "{'TP': 16, 'FP': 2, 'FN': 0, 'Precision': 0.8888888395061756, 'Recall': 0.9999999375000038, 'F1-Score': 0.9411759169552841}\n",
      "Processing folder 1238, fold 3\n",
      "Precision: 0.9999999996483826, Recall: 0.7364060070594495, F1: 0.8481951569600226\n",
      "Avg Precision: 1.0000, Avg Recall: 0.7364, Avg F1-Score: 0.8482\n",
      "20\n",
      "19\n",
      "{'TP': 19, 'FP': 2, 'FN': 0, 'Precision': 0.9047618616780065, 'Recall': 0.9999999473684238, 'F1-Score': 0.9499994537502641}\n",
      "Processing folder 1238, fold 4\n",
      "Precision: 0.9776006070700074, Recall: 0.8322559790458126, F1: 0.8990916816819237\n",
      "Avg Precision: 0.9776, Avg Recall: 0.8323, Avg F1-Score: 0.8991\n",
      "18\n",
      "18\n",
      "{'TP': 17, 'FP': 2, 'FN': 1, 'Precision': 0.8947367950138528, 'Recall': 0.9444443919753115, 'F1-Score': 0.9189183696131303}\n",
      "Results saved to raw_data_exp_25_04_24/1238/final_results.csv\n",
      "Processing folder 1275, fold 0\n",
      "Precision: 0.8750678977231453, Recall: 0.6217676571951926, F1: 0.7269850737761092\n",
      "Avg Precision: 0.8751, Avg Recall: 0.6218, Avg F1-Score: 0.7270\n",
      "51\n",
      "78\n",
      "{'TP': 59, 'FP': 9, 'FN': 19, 'Precision': 0.8676470460640141, 'Recall': 0.7564102467126892, 'F1-Score': 0.8082186693566586}\n",
      "Processing folder 1275, fold 1\n",
      "Precision: 0.947510822339915, Recall: 0.5478152048651772, F1: 0.6942439022483088\n",
      "Avg Precision: 0.9475, Avg Recall: 0.5478, Avg F1-Score: 0.6942\n",
      "54\n",
      "81\n",
      "{'TP': 58, 'FP': 13, 'FN': 23, 'Precision': 0.8169013969450508, 'Recall': 0.7160493738759337, 'F1-Score': 0.7631573868597432}\n",
      "Processing folder 1275, fold 2\n",
      "Precision: 0.9315044856915968, Recall: 0.647517390183795, F1: 0.7639729135464655\n",
      "Avg Precision: 0.9315, Avg Recall: 0.6475, Avg F1-Score: 0.7640\n",
      "58\n",
      "91\n",
      "{'TP': 65, 'FP': 11, 'FN': 26, 'Precision': 0.8552631466412745, 'Recall': 0.7142857064364209, 'F1-Score': 0.7784426084839476}\n",
      "Processing folder 1275, fold 3\n",
      "Precision: 0.9343669248231611, Recall: 0.5756128620541845, F1: 0.7123714744986548\n",
      "Avg Precision: 0.9344, Avg Recall: 0.5756, Avg F1-Score: 0.7124\n",
      "49\n",
      "58\n",
      "{'TP': 47, 'FP': 12, 'FN': 11, 'Precision': 0.7966101559896585, 'Recall': 0.8103448136147446, 'F1-Score': 0.8034182897219858}\n",
      "Processing folder 1275, fold 4\n",
      "Precision: 0.9262340541826152, Recall: 0.6364329267484206, F1: 0.7544607775303388\n",
      "Avg Precision: 0.9262, Avg Recall: 0.6364, Avg F1-Score: 0.7545\n",
      "51\n",
      "79\n",
      "{'TP': 59, 'FP': 8, 'FN': 20, 'Precision': 0.8805970017821343, 'Recall': 0.7468354335843617, 'F1-Score': 0.808218670388746}\n",
      "Results saved to raw_data_exp_25_04_24/1275/final_results.csv\n",
      "Processing folder 1239, fold 0\n",
      "Precision: 0.8451337637889106, Recall: 0.6447611506710319, F1: 0.7314731354357689\n",
      "Avg Precision: 0.8451, Avg Recall: 0.6448, Avg F1-Score: 0.7315\n",
      "88\n",
      "576\n",
      "{'TP': 324, 'FP': 5, 'FN': 252, 'Precision': 0.9848024286176218, 'Recall': 0.5624999990234375, 'F1-Score': 0.7160216351103383}\n",
      "Processing folder 1239, fold 1\n",
      "Precision: 0.8772910437490387, Recall: 0.6984734799121101, F1: 0.7777356707226534\n",
      "Avg Precision: 0.8773, Avg Recall: 0.6985, Avg F1-Score: 0.7777\n",
      "78\n",
      "498\n",
      "{'TP': 313, 'FP': 5, 'FN': 185, 'Precision': 0.9842767264645386, 'Recall': 0.6285140549628232, 'F1-Score': 0.7671563851946911}\n",
      "Processing folder 1239, fold 2\n",
      "Precision: 0.8958765738967845, Recall: 0.7100400284745276, F1: 0.7922053125266854\n",
      "Avg Precision: 0.8959, Avg Recall: 0.7100, Avg F1-Score: 0.7922\n",
      "77\n",
      "459\n",
      "{'TP': 323, 'FP': 2, 'FN': 136, 'Precision': 0.9938461507881657, 'Recall': 0.7037037021705802, 'F1-Score': 0.8239791043415761}\n",
      "Processing folder 1239, fold 3\n",
      "Precision: 0.8080472498480585, Recall: 0.7090108181182218, F1: 0.7552958929423642\n",
      "Avg Precision: 0.8080, Avg Recall: 0.7090, Avg F1-Score: 0.7553\n",
      "72\n",
      "471\n",
      "{'TP': 258, 'FP': 5, 'FN': 213, 'Precision': 0.980988589425899, 'Recall': 0.5477706994739475, 'F1-Score': 0.7029968134408934}\n",
      "Processing folder 1239, fold 4\n",
      "Precision: 0.8934099955207297, Recall: 0.6707143534450851, F1: 0.7662082938599049\n",
      "Avg Precision: 0.8934, Avg Recall: 0.6707, Avg F1-Score: 0.7662\n",
      "81\n",
      "501\n",
      "{'TP': 305, 'FP': 10, 'FN': 196, 'Precision': 0.9682539651801462, 'Recall': 0.608782433914606, 'F1-Score': 0.7475485437545056}\n",
      "Results saved to raw_data_exp_25_04_24/1239/final_results.csv\n",
      "Processing folder 1520, fold 0\n",
      "Precision: 0.999999999547716, Recall: 0.7299438756256376, F1: 0.8438926416338552\n",
      "Avg Precision: 1.0000, Avg Recall: 0.7299, Avg F1-Score: 0.8439\n",
      "31\n",
      "30\n",
      "{'TP': 30, 'FP': 1, 'FN': 0, 'Precision': 0.9677419042663902, 'Recall': 0.9999999666666678, 'F1-Score': 0.9836060252622816}\n",
      "Processing folder 1520, fold 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9900819314402407, Recall: 0.8961748630381832, F1: 0.9407903224070011\n",
      "Avg Precision: 0.9901, Avg Recall: 0.8962, Avg F1-Score: 0.9408\n",
      "38\n",
      "37\n",
      "{'TP': 36, 'FP': 3, 'FN': 1, 'Precision': 0.9230768994082847, 'Recall': 0.9729729466764069, 'F1-Score': 0.9473678964684084}\n",
      "Processing folder 1520, fold 2\n",
      "Precision: 0.8942206651859123, Recall: 0.972201065890251, F1: 0.9315813286667017\n",
      "Avg Precision: 0.8942, Avg Recall: 0.9722, Avg F1-Score: 0.9316\n",
      "39\n",
      "43\n",
      "{'TP': 38, 'FP': 2, 'FN': 5, 'Precision': 0.9499999762500007, 'Recall': 0.8837209096809091, 'F1-Score': 0.9156621291917375}\n",
      "Processing folder 1520, fold 3\n",
      "Precision: 0.9490777863075069, Recall: 0.8444523721568133, F1: 0.893712924056407\n",
      "Avg Precision: 0.9491, Avg Recall: 0.8445, Avg F1-Score: 0.8937\n",
      "40\n",
      "40\n",
      "{'TP': 38, 'FP': 3, 'FN': 2, 'Precision': 0.9268292456870916, 'Recall': 0.9499999762500007, 'F1-Score': 0.9382710818475464}\n",
      "Processing folder 1520, fold 4\n",
      "Precision: 0.9988085778400284, Recall: 0.973297213245628, F1: 0.9858873868005505\n",
      "Avg Precision: 0.9988, Avg Recall: 0.9733, Avg F1-Score: 0.9859\n",
      "38\n",
      "38\n",
      "{'TP': 37, 'FP': 1, 'FN': 1, 'Precision': 0.9736841849030479, 'Recall': 0.9736841849030479, 'F1-Score': 0.9736836849033047}\n",
      "Results saved to raw_data_exp_25_04_24/1520/final_results.csv\n",
      "Processing folder 1509, fold 0\n",
      "Precision: 0.9449431472501836, Recall: 0.5244104946780437, F1: 0.6744976184126711\n",
      "Avg Precision: 0.9449, Avg Recall: 0.5244, Avg F1-Score: 0.6745\n",
      "41\n",
      "38\n",
      "{'TP': 37, 'FP': 4, 'FN': 1, 'Precision': 0.9024390023795366, 'Recall': 0.9736841849030479, 'F1-Score': 0.9367083377666504}\n",
      "Processing folder 1509, fold 1\n",
      "Precision: 0.9677669899154303, Recall: 0.5791308388149823, F1: 0.7246287840390485\n",
      "Avg Precision: 0.9678, Avg Recall: 0.5791, Avg F1-Score: 0.7246\n",
      "52\n",
      "49\n",
      "{'TP': 48, 'FP': 6, 'FN': 1, 'Precision': 0.8888888724279839, 'Recall': 0.9795918167430242, 'F1-Score': 0.932038318032127}\n",
      "Processing folder 1509, fold 2\n",
      "Precision: 0.9197267288984942, Recall: 0.7062295079651707, F1: 0.7989609326568811\n",
      "Avg Precision: 0.9197, Avg Recall: 0.7062, Avg F1-Score: 0.7990\n",
      "44\n",
      "48\n",
      "{'TP': 44, 'FP': 2, 'FN': 4, 'Precision': 0.9565217183364845, 'Recall': 0.9166666475694449, 'F1-Score': 0.9361696930740564}\n",
      "Processing folder 1509, fold 3\n",
      "Precision: 0.99476439738494, Recall: 0.7943143809388318, F1: 0.8833095941972169\n",
      "Avg Precision: 0.9948, Avg Recall: 0.7943, Avg F1-Score: 0.8833\n",
      "42\n",
      "38\n",
      "{'TP': 38, 'FP': 4, 'FN': 0, 'Precision': 0.9047618832199552, 'Recall': 0.9999999736842113, 'F1-Score': 0.9499994775002626}\n",
      "Processing folder 1509, fold 4\n",
      "Precision: 0.99478623514349, Recall: 0.8033684207143712, F1: 0.8888883941411059\n",
      "Avg Precision: 0.9948, Avg Recall: 0.8034, Avg F1-Score: 0.8889\n",
      "42\n",
      "41\n",
      "{'TP': 41, 'FP': 3, 'FN': 0, 'Precision': 0.9318181606404964, 'Recall': 0.9999999756097567, 'F1-Score': 0.9647053602770757}\n",
      "Results saved to raw_data_exp_25_04_24/1509/final_results.csv\n",
      "Processing folder 1240, fold 0\n",
      "Precision: 0.928167999045902, Recall: 0.9361540281798547, F1: 0.9321434091037892\n",
      "Avg Precision: 0.9282, Avg Recall: 0.9362, Avg F1-Score: 0.9321\n",
      "358\n",
      "610\n",
      "{'TP': 485, 'FP': 8, 'FN': 125, 'Precision': 0.9837728174771343, 'Recall': 0.7950819659097017, 'F1-Score': 0.8794192683108033}\n",
      "Processing folder 1240, fold 1\n",
      "Precision: 0.9651156310770852, Recall: 0.9444666618020711, F1: 0.9546790042994551\n",
      "Avg Precision: 0.9651, Avg Recall: 0.9445, Avg F1-Score: 0.9547\n",
      "338\n",
      "464\n",
      "{'TP': 444, 'FP': 6, 'FN': 20, 'Precision': 0.9866666644740741, 'Recall': 0.9568965496618609, 'F1-Score': 0.9715531084949113}\n",
      "Processing folder 1240, fold 2\n",
      "Precision: 0.9603466259975874, Recall: 0.950478066120033, F1: 0.9553863627105753\n",
      "Avg Precision: 0.9603, Avg Recall: 0.9505, Avg F1-Score: 0.9554\n",
      "369\n",
      "503\n",
      "{'TP': 480, 'FP': 4, 'FN': 23, 'Precision': 0.9917355351410423, 'Recall': 0.9542743519795739, 'F1-Score': 0.9726438751143285}\n",
      "Processing folder 1240, fold 3\n",
      "Precision: 0.9718519787395605, Recall: 0.9620126511275457, F1: 0.9669067841440916\n",
      "Avg Precision: 0.9719, Avg Recall: 0.9620, Avg F1-Score: 0.9669\n",
      "334\n",
      "493\n",
      "{'TP': 461, 'FP': 5, 'FN': 32, 'Precision': 0.9892703841431966, 'Recall': 0.9350912759937297, 'F1-Score': 0.9614176422914454}\n",
      "Processing folder 1240, fold 4\n",
      "Precision: 0.9848289910835062, Recall: 0.9502241379146513, F1: 0.9672166426994784\n",
      "Avg Precision: 0.9848, Avg Recall: 0.9502, Avg F1-Score: 0.9672\n",
      "334\n",
      "449\n",
      "{'TP': 434, 'FP': 5, 'FN': 15, 'Precision': 0.9886104761079488, 'Recall': 0.9665924254641595, 'F1-Score': 0.9774769753396154}\n",
      "Results saved to raw_data_exp_25_04_24/1240/final_results.csv\n",
      "Processing folder 1308, fold 0\n",
      "Precision: 0.9532464195650089, Recall: 0.7257480930605443, F1: 0.8240841217428371\n",
      "Avg Precision: 0.9532, Avg Recall: 0.7257, Avg F1-Score: 0.8241\n",
      "85\n",
      "143\n",
      "{'TP': 114, 'FP': 9, 'FN': 29, 'Precision': 0.9268292607574857, 'Recall': 0.7972027916279525, 'F1-Score': 0.857142353525078}\n",
      "Processing folder 1308, fold 1\n",
      "Precision: 0.9181253879007991, Recall: 0.7950440764999439, F1: 0.8521628948232842\n",
      "Avg Precision: 0.9181, Avg Recall: 0.7950, Avg F1-Score: 0.8522\n",
      "72\n",
      "157\n",
      "{'TP': 110, 'FP': 5, 'FN': 47, 'Precision': 0.9565217308128545, 'Recall': 0.7006369382125036, 'F1-Score': 0.8088230353863238}\n",
      "Processing folder 1308, fold 2\n",
      "Precision: 0.9604278074181513, Recall: 0.7351015061812322, F1: 0.832791867121093\n",
      "Avg Precision: 0.9604, Avg Recall: 0.7351, Avg F1-Score: 0.8328\n",
      "86\n",
      "118\n",
      "{'TP': 101, 'FP': 6, 'FN': 17, 'Precision': 0.9439252248231288, 'Recall': 0.8559321961361679, 'F1-Score': 0.8977772709928699}\n",
      "Processing folder 1308, fold 3\n",
      "Precision: 0.8916891111394325, Recall: 0.758799454256, F1: 0.8198939576288843\n",
      "Avg Precision: 0.8917, Avg Recall: 0.7588, Avg F1-Score: 0.8199\n",
      "83\n",
      "161\n",
      "{'TP': 115, 'FP': 3, 'FN': 46, 'Precision': 0.9745762629273198, 'Recall': 0.7142857098491571, 'F1-Score': 0.8243722658242121}\n",
      "Processing folder 1308, fold 4\n",
      "Precision: 0.9333483111710216, Recall: 0.7756893010035669, F1: 0.8472462751018872\n",
      "Avg Precision: 0.9333, Avg Recall: 0.7757, Avg F1-Score: 0.8472\n",
      "72\n",
      "108\n",
      "{'TP': 89, 'FP': 4, 'FN': 19, 'Precision': 0.9569892370216211, 'Recall': 0.8240740664437587, 'F1-Score': 0.8855716332766833}\n",
      "Results saved to raw_data_exp_25_04_24/1308/final_results.csv\n",
      "Processing folder 1319, fold 0\n",
      "Precision: 0.9247452838772576, Recall: 0.5838481625002325, F1: 0.7157799580516192\n",
      "Avg Precision: 0.9247, Avg Recall: 0.5838, Avg F1-Score: 0.7158\n",
      "82\n",
      "144\n",
      "{'TP': 122, 'FP': 8, 'FN': 22, 'Precision': 0.9384615312426037, 'Recall': 0.8472222163387346, 'F1-Score': 0.8905104437106529}\n",
      "Processing folder 1319, fold 1\n",
      "Precision: 0.9019189763535354, Recall: 0.630308448721456, F1: 0.7420396874963563\n",
      "Avg Precision: 0.9019, Avg Recall: 0.6303, Avg F1-Score: 0.7420\n",
      "44\n",
      "76\n",
      "{'TP': 60, 'FP': 6, 'FN': 16, 'Precision': 0.9090908953168046, 'Recall': 0.7894736738227148, 'F1-Score': 0.8450699131127707}\n",
      "Processing folder 1319, fold 2\n",
      "Precision: 0.8788169244312177, Recall: 0.5196761133182448, F1: 0.653131370286937\n",
      "Avg Precision: 0.8788, Avg Recall: 0.5197, Avg F1-Score: 0.6531\n",
      "62\n",
      "120\n",
      "{'TP': 102, 'FP': 4, 'FN': 18, 'Precision': 0.9622641418654326, 'Recall': 0.8499999929166667, 'F1-Score': 0.9026543611875253}\n",
      "Processing folder 1319, fold 3\n",
      "Precision: 0.9248050881283539, Recall: 0.6189920351126756, F1: 0.7416086014942673\n",
      "Avg Precision: 0.9248, Avg Recall: 0.6190, Avg F1-Score: 0.7416\n",
      "73\n",
      "137\n",
      "{'TP': 116, 'FP': 6, 'FN': 21, 'Precision': 0.9508196643375437, 'Recall': 0.8467153222867495, 'F1-Score': 0.8957523905132394}\n",
      "Processing folder 1319, fold 4\n",
      "Precision: 0.9404589370844856, Recall: 0.4587604571427618, F1: 0.6166939442576528\n",
      "Avg Precision: 0.9405, Avg Recall: 0.4588, Avg F1-Score: 0.6167\n",
      "77\n",
      "127\n",
      "{'TP': 118, 'FP': 10, 'FN': 9, 'Precision': 0.9218749927978517, 'Recall': 0.929133850951702, 'F1-Score': 0.9254896888276444}\n",
      "Results saved to raw_data_exp_25_04_24/1319/final_results.csv\n",
      "Processing folder 1439, fold 0\n",
      "Precision: 0.9351259065577782, Recall: 0.7026940344122213, F1: 0.802416649494418\n",
      "Avg Precision: 0.9351, Avg Recall: 0.7027, Avg F1-Score: 0.8024\n",
      "22\n",
      "41\n",
      "{'TP': 32, 'FP': 5, 'FN': 9, 'Precision': 0.8648648414901394, 'Recall': 0.7804877858417614, 'F1-Score': 0.8205123007892583}\n",
      "Processing folder 1439, fold 1\n",
      "Precision: 0.9137151466037245, Recall: 0.7155503096147683, F1: 0.802581006386741\n",
      "Avg Precision: 0.9137, Avg Recall: 0.7156, Avg F1-Score: 0.8026\n",
      "27\n",
      "49\n",
      "{'TP': 39, 'FP': 1, 'FN': 10, 'Precision': 0.9749999756250006, 'Recall': 0.7959183511037071, 'F1-Score': 0.87640397980081}\n",
      "Processing folder 1439, fold 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9341816582122942, Recall: 0.5200293110600808, F1: 0.6681307187537144\n",
      "Avg Precision: 0.9342, Avg Recall: 0.5200, Avg F1-Score: 0.6681\n",
      "27\n",
      "35\n",
      "{'TP': 32, 'FP': 8, 'FN': 3, 'Precision': 0.7999999800000005, 'Recall': 0.9142856881632662, 'F1-Score': 0.8533328128002909}\n",
      "Processing folder 1439, fold 3\n",
      "Precision: 0.9694357362972431, Recall: 0.6242745393327594, F1: 0.7594776506182896\n",
      "Avg Precision: 0.9694, Avg Recall: 0.6243, Avg F1-Score: 0.7595\n",
      "24\n",
      "41\n",
      "{'TP': 38, 'FP': 3, 'FN': 3, 'Precision': 0.9268292456870916, 'Recall': 0.9268292456870916, 'F1-Score': 0.9268287456873614}\n",
      "Processing folder 1439, fold 4\n",
      "Precision: 0.9484184910226675, Recall: 0.5703833770645644, F1: 0.7123533318860146\n",
      "Avg Precision: 0.9484, Avg Recall: 0.5704, Avg F1-Score: 0.7124\n",
      "27\n",
      "40\n",
      "{'TP': 36, 'FP': 6, 'FN': 4, 'Precision': 0.8571428367346944, 'Recall': 0.8999999775000006, 'F1-Score': 0.878048259369708}\n",
      "Results saved to raw_data_exp_25_04_24/1439/final_results.csv\n",
      "Processing folder 1267, fold 0\n",
      "Precision: 0.9508925957886513, Recall: 0.7943865825243565, F1: 0.8656218398657454\n",
      "Avg Precision: 0.9509, Avg Recall: 0.7944, Avg F1-Score: 0.8656\n",
      "33\n",
      "99\n",
      "{'TP': 86, 'FP': 2, 'FN': 13, 'Precision': 0.9772727161673556, 'Recall': 0.8686868599122539, 'F1-Score': 0.9197855881497738}\n",
      "Processing folder 1267, fold 1\n",
      "Precision: 0.9789419239708185, Recall: 0.7746141253055067, F1: 0.864873193004458\n",
      "Avg Precision: 0.9789, Avg Recall: 0.7746, Avg F1-Score: 0.8649\n",
      "32\n",
      "77\n",
      "{'TP': 68, 'FP': 2, 'FN': 9, 'Precision': 0.9714285575510206, 'Recall': 0.8831168716478328, 'F1-Score': 0.925169556573919}\n",
      "Processing folder 1267, fold 2\n",
      "Precision: 0.9829814733969214, Recall: 0.8430484987673857, F1: 0.9076527924180912\n",
      "Avg Precision: 0.9830, Avg Recall: 0.8430, Avg F1-Score: 0.9077\n",
      "27\n",
      "62\n",
      "{'TP': 52, 'FP': 3, 'FN': 10, 'Precision': 0.9454545282644632, 'Recall': 0.8387096638917797, 'F1-Score': 0.8888883754842448}\n",
      "Processing folder 1267, fold 3\n",
      "Precision: 0.9584560989040998, Recall: 0.9339649726352794, F1: 0.9460515575755447\n",
      "Avg Precision: 0.9585, Avg Recall: 0.9340, Avg F1-Score: 0.9461\n",
      "26\n",
      "52\n",
      "{'TP': 43, 'FP': 0, 'FN': 9, 'Precision': 0.9999999767441866, 'Recall': 0.8269230610207104, 'F1-Score': 0.9052626433243712}\n",
      "Processing folder 1267, fold 4\n",
      "Precision: 0.9409914442701319, Recall: 0.7713490098214367, F1: 0.8477664510823617\n",
      "Avg Precision: 0.9410, Avg Recall: 0.7713, Avg F1-Score: 0.8478\n",
      "24\n",
      "50\n",
      "{'TP': 45, 'FP': 2, 'FN': 5, 'Precision': 0.9574467881394301, 'Recall': 0.8999999820000004, 'F1-Score': 0.9278345328943072}\n",
      "Results saved to raw_data_exp_25_04_24/1267/final_results.csv\n",
      "Processing folder 1242, fold 0\n",
      "Precision: 0.9073282943717279, Recall: 0.7501698138330274, F1: 0.8212979353111\n",
      "Avg Precision: 0.9073, Avg Recall: 0.7502, Avg F1-Score: 0.8213\n",
      "37\n",
      "84\n",
      "{'TP': 70, 'FP': 0, 'FN': 14, 'Precision': 0.9999999857142859, 'Recall': 0.8333333234126985, 'F1-Score': 0.9090904014170355}\n",
      "Processing folder 1242, fold 1\n",
      "Precision: 0.942042885459886, Recall: 0.842224535993004, F1: 0.8893411132007881\n",
      "Avg Precision: 0.9420, Avg Recall: 0.8422, Avg F1-Score: 0.8893\n",
      "37\n",
      "90\n",
      "{'TP': 76, 'FP': 1, 'FN': 14, 'Precision': 0.9870129741946367, 'Recall': 0.8444444350617285, 'F1-Score': 0.9101791328483478}\n",
      "Processing folder 1242, fold 2\n",
      "Precision: 0.947542309962407, Recall: 0.8328690807305832, F1: 0.8865122448901304\n",
      "Avg Precision: 0.9475, Avg Recall: 0.8329, Avg F1-Score: 0.8865\n",
      "37\n",
      "95\n",
      "{'TP': 79, 'FP': 2, 'FN': 16, 'Precision': 0.9753086299344614, 'Recall': 0.8315789386149586, 'F1-Score': 0.8977267656898411}\n",
      "Processing folder 1242, fold 3\n",
      "Precision: 0.8581882560721046, Recall: 0.8919830705457902, F1: 0.8747588841571915\n",
      "Avg Precision: 0.8582, Avg Recall: 0.8920, Avg F1-Score: 0.8748\n",
      "41\n",
      "103\n",
      "{'TP': 78, 'FP': 3, 'FN': 25, 'Precision': 0.9629629510745314, 'Recall': 0.7572815460458102, 'F1-Score': 0.8478255848892279}\n",
      "Processing folder 1242, fold 4\n",
      "Precision: 0.8711045155203798, Recall: 0.7639439761549365, F1: 0.8140121147663789\n",
      "Avg Precision: 0.8711, Avg Recall: 0.7639, Avg F1-Score: 0.8140\n",
      "38\n",
      "96\n",
      "{'TP': 81, 'FP': 2, 'FN': 15, 'Precision': 0.9759036026999566, 'Recall': 0.8437499912109376, 'F1-Score': 0.9050274254863694}\n",
      "Results saved to raw_data_exp_25_04_24/1242/final_results.csv\n",
      "Processing folder 1462, fold 0\n",
      "Precision: 0.9513417869899399, Recall: 0.8358234734574813, F1: 0.8898487112755926\n",
      "Avg Precision: 0.9513, Avg Recall: 0.8358, Avg F1-Score: 0.8898\n",
      "68\n",
      "76\n",
      "{'TP': 70, 'FP': 5, 'FN': 6, 'Precision': 0.9333333208888891, 'Recall': 0.921052619459834, 'F1-Score': 0.9271518056228303}\n",
      "Processing folder 1462, fold 1\n",
      "Precision: 0.8965953218733319, Recall: 0.9024634123350183, F1: 0.8995192969703959\n",
      "Avg Precision: 0.8966, Avg Recall: 0.9025, Avg F1-Score: 0.8995\n",
      "60\n",
      "74\n",
      "{'TP': 60, 'FP': 4, 'FN': 14, 'Precision': 0.9374999853515628, 'Recall': 0.8108107998539081, 'F1-Score': 0.8695647074146934}\n",
      "Processing folder 1462, fold 2\n",
      "Precision: 0.9568418898591898, Recall: 0.860875966059912, F1: 0.9063251818253483\n",
      "Avg Precision: 0.9568, Avg Recall: 0.8609, Avg F1-Score: 0.9063\n",
      "64\n",
      "71\n",
      "{'TP': 66, 'FP': 5, 'FN': 5, 'Precision': 0.9295774516960923, 'Recall': 0.9295774516960923, 'F1-Score': 0.9295769516963613}\n",
      "Processing folder 1462, fold 3\n",
      "Precision: 0.9434933107920765, Recall: 0.820919820852998, F1: 0.8779484881469274\n",
      "Avg Precision: 0.9435, Avg Recall: 0.8209, Avg F1-Score: 0.8779\n",
      "63\n",
      "70\n",
      "{'TP': 63, 'FP': 5, 'FN': 7, 'Precision': 0.9264705746107269, 'Recall': 0.8999999871428573, 'F1-Score': 0.9130429651336494}\n",
      "Processing folder 1462, fold 4\n",
      "Precision: 0.8654752552174514, Recall: 0.8613309878958926, F1: 0.8633976485202485\n",
      "Avg Precision: 0.8655, Avg Recall: 0.8613, Avg F1-Score: 0.8634\n",
      "59\n",
      "73\n",
      "{'TP': 60, 'FP': 6, 'FN': 13, 'Precision': 0.9090908953168046, 'Recall': 0.8219177969600302, 'F1-Score': 0.863308841364607}\n",
      "Results saved to raw_data_exp_25_04_24/1462/final_results.csv\n",
      "Processing folder 1265, fold 0\n",
      "Precision: 0.9261378411115376, Recall: 0.6811400151719319, F1: 0.7849658952208055\n",
      "Avg Precision: 0.9261, Avg Recall: 0.6811, Avg F1-Score: 0.7850\n",
      "68\n",
      "78\n",
      "{'TP': 64, 'FP': 7, 'FN': 14, 'Precision': 0.9014084380083319, 'Recall': 0.8205128099934256, 'F1-Score': 0.8590598922573957}\n",
      "Processing folder 1265, fold 1\n"
     ]
    }
   ],
   "source": [
    "path = 'core/qrels/'\n",
    "model_save_dir = 'raw_data_exp_25_04_24'\n",
    "folders_to_process = ['1272', '1474','1238', '1275', '1239', '1520', '1509', '1240', '1308', '1319', '1439', \n",
    "                 '1267', '1242', '1462', '1265', '1444', '1312', '1244', '1243', '1468', '1309', '1524', \n",
    "                 '1247', '1440', '1251', '1249', '1248', '1262', '1250', '1252', '1245', '1512', '1498', \n",
    "                 '1601', '1443', '1086', '1551', '1253', '1320', '1304', '1469', '1611', '1300', '1489', \n",
    "                 '1500', '1261', '1318', '1460', '1475', '1321']\n",
    "\n",
    "\n",
    "results_df = test_crf_model(model_save_dir, folders_to_process, path, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9cb75f",
   "metadata": {},
   "source": [
    "### Calculating mean across topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ea7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "# Step 1: Loop through each topic ID and read the CSV file\n",
    "for topic_id in folders_to_process:\n",
    "    csv_path = os.path.join('./',str(topic_id), \"final_results.csv\")\n",
    "    print(csv_path)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Step 2: Concatenate all DataFrames\n",
    "combined_df = pd.concat(dataframes)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean for each topic across all folds\n",
    "topic_means = combined_df.groupby('Topic ID').mean().drop(columns='Fold')\n",
    "\n",
    "# Calculate the mean of means across all topics\n",
    "overall_means = topic_means.mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean Metrics for Each Topic:\")\n",
    "print(topic_means)\n",
    "print(\"\\nOverall Mean Metrics Across All Topics:\")\n",
    "print(overall_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7912850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
